{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nThis notebook uses approach from paper: https://arxiv.org/abs/2106.12978\\n'"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This notebook uses approach from paper: https://arxiv.org/abs/2106.12978\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "PARALLEL_INFERENCE_INSTANCES = 20\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "from enum import Enum\n",
    "from typing import NamedTuple, Optional\n",
    "\n",
    "\n",
    "class TopicSegmentationAlgorithm(Enum):\n",
    "    RANDOM = 0\n",
    "    EVEN = 1\n",
    "    BERT = 2\n",
    "    SBERT = 3\n",
    "\n",
    "\n",
    "class TextTilingHyperparameters(NamedTuple):\n",
    "    SENTENCE_COMPARISON_WINDOW: int = 15\n",
    "    SMOOTHING_PASSES: int = 2\n",
    "    SMOOTHING_WINDOW: int = 1\n",
    "    TOPIC_CHANGE_THRESHOLD: float = 0.6\n",
    "\n",
    "\n",
    "class TopicSegmentationConfig(NamedTuple):\n",
    "    TEXT_TILING: Optional[TextTilingHyperparameters] = None\n",
    "    MAX_SEGMENTS_CAP: bool = True\n",
    "    MAX_SEGMENTS_CAP__AVERAGE_SEGMENT_LENGTH: int = 60\n",
    "\n",
    "\n",
    "def PrintMessage(msg, x):\n",
    "    print(msg)\n",
    "    print(x)\n",
    "\n",
    "\n",
    "def depth_score(timeseries):\n",
    "    \"\"\"\n",
    "    The depth score corresponds to how strongly the cues for a subtopic changed on both sides of a\n",
    "    given token-sequence gap and is based on the distance from the peaks on both sides of the valleyto that valley.\n",
    "\n",
    "    returns depth_scores\n",
    "    \"\"\"\n",
    "    depth_scores = []\n",
    "    for i in range(1, len(timeseries) - 1):\n",
    "        left, right = i - 1, i + 1\n",
    "        while left > 0 and timeseries[left - 1] > timeseries[left]:\n",
    "            left -= 1\n",
    "        while (\n",
    "                right < (len(timeseries) - 1) and timeseries[right + 1] > timeseries[right]\n",
    "        ):\n",
    "            right += 1\n",
    "        depth_scores.append(\n",
    "            (timeseries[right] - timeseries[i]) + (timeseries[left] - timeseries[i])\n",
    "        )\n",
    "    return depth_scores\n",
    "\n",
    "\n",
    "def smooth(timeseries, n, s):\n",
    "    smoothed_timeseries = timeseries[:]\n",
    "    for _ in range(n):\n",
    "        for index in range(len(smoothed_timeseries)):\n",
    "            neighbours = smoothed_timeseries[\n",
    "                         max(0, index - s): min(len(timeseries) - 1, index + s)\n",
    "                         ]\n",
    "            smoothed_timeseries[index] = sum(neighbours) / len(neighbours)\n",
    "    return smoothed_timeseries\n",
    "\n",
    "\n",
    "def sentences_similarity(first_sentence_features, second_sentence_features) -> float:\n",
    "    \"\"\"\n",
    "    Given two senteneces embedding features compute cosine similarity\n",
    "    \"\"\"\n",
    "    similarity_metric = torch.nn.CosineSimilarity()\n",
    "    return float(similarity_metric(first_sentence_features, second_sentence_features))\n",
    "\n",
    "\n",
    "def compute_window(timeseries, start_index, end_index):\n",
    "    \"\"\"given start and end index of embedding, compute pooled window value\n",
    "\n",
    "    [window_size, 768] -> [1, 768]\n",
    "    \"\"\"\n",
    "    stack = torch.stack(timeseries[start_index:end_index])\n",
    "    stack = stack.unsqueeze(\n",
    "        0\n",
    "    )  # https://jbencook.com/adding-a-dimension-to-a-tensor-in-pytorch/\n",
    "    stack_size = end_index - start_index\n",
    "    pooling = torch.nn.MaxPool2d((stack_size - 1, 1))\n",
    "    return pooling(stack)\n",
    "\n",
    "\n",
    "def block_comparison_score(timeseries, k):\n",
    "    \"\"\"\n",
    "    comparison score for a gap (i)\n",
    "\n",
    "    cfr. docstring of block_comparison_score\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for i in range(k, len(timeseries) - k):\n",
    "        first_window_features = compute_window(timeseries, i - k, i + 1)\n",
    "        second_window_features = compute_window(timeseries, i + 1, i + k + 2)\n",
    "        res.append(\n",
    "            sentences_similarity(first_window_features[0], second_window_features[0])\n",
    "        )\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_features_from_sentence(batch_sentences, layer=-2):\n",
    "    \"\"\"\n",
    "    extracts the BERT semantic representation\n",
    "    from a sentence, using an averaged value of\n",
    "    the `layer`-th layer\n",
    "\n",
    "    returns a 1-dimensional tensor of size 758\n",
    "    \"\"\"\n",
    "\n",
    "    return model.encode(batch_sentences, convert_to_numpy=False)\n",
    "\n",
    "\n",
    "def arsort2(array1, array2):\n",
    "    x = np.array(array1)\n",
    "    y = np.array(array2)\n",
    "\n",
    "    sorted_idx = x.argsort()[::-1]\n",
    "    return x[sorted_idx], y[sorted_idx]\n",
    "\n",
    "\n",
    "def get_local_maxima(array):\n",
    "    local_maxima_indices = []\n",
    "    local_maxima_values = []\n",
    "    for i in range(1, len(array) - 1):\n",
    "        if array[i - 1] < array[i] and array[i] > array[i + 1]:\n",
    "            local_maxima_indices.append(i)\n",
    "            local_maxima_values.append(array[i])\n",
    "    return local_maxima_indices, local_maxima_values\n",
    "\n",
    "\n",
    "def depth_score_to_topic_change_indexes(\n",
    "        depth_score_timeseries,\n",
    "        meeting_duration=60 * 3600,\n",
    "        topic_segmentation_configs=TopicSegmentationConfig,\n",
    "):\n",
    "    \"\"\"\n",
    "    capped add a max segment limit so there are not too many segments, used for UI improvements on the Workplace TeamWork product\n",
    "    \"\"\"\n",
    "\n",
    "    capped = topic_segmentation_configs.MAX_SEGMENTS_CAP\n",
    "    average_segment_length = (\n",
    "        topic_segmentation_configs.MAX_SEGMENTS_CAP__AVERAGE_SEGMENT_LENGTH\n",
    "    )\n",
    "    threshold = topic_segmentation_configs.TEXT_TILING.TOPIC_CHANGE_THRESHOLD * max(\n",
    "        depth_score_timeseries\n",
    "    )\n",
    "\n",
    "    # print(\"DEPTH_SCORE_TIMESERIES:\")\n",
    "    # print(list(depth_score_timeseries))\n",
    "\n",
    "    if depth_score_timeseries == []:\n",
    "        return []\n",
    "\n",
    "    local_maxima_indices, local_maxima = get_local_maxima(depth_score_timeseries)\n",
    "\n",
    "    if local_maxima == []:\n",
    "        return []\n",
    "\n",
    "    if capped:  # capped is segmentation used for UI\n",
    "        # sort based on maxima for pruning\n",
    "        local_maxima, local_maxima_indices = arsort2(local_maxima, local_maxima_indices)\n",
    "\n",
    "        # local maxima are sorted by depth_score value and we take only the first K\n",
    "        # where the K+1th local maxima is lower then the threshold\n",
    "        for thres in range(len(local_maxima)):\n",
    "            if local_maxima[thres] <= threshold:\n",
    "                break\n",
    "\n",
    "        max_segments = int(meeting_duration / average_segment_length)\n",
    "        slice_length = min(max_segments, thres)\n",
    "\n",
    "        local_maxima_indices = local_maxima_indices[:slice_length]\n",
    "        local_maxima = local_maxima[:slice_length]\n",
    "\n",
    "        # after pruning, sort again based on indices for chronological ordering\n",
    "        local_maxima_indices, _ = arsort2(local_maxima_indices, local_maxima)\n",
    "\n",
    "    else:  # this is the vanilla TextTiling used for Pk optimization\n",
    "        filtered_local_maxima_indices = []\n",
    "        filtered_local_maxima = []\n",
    "\n",
    "        for i, m in enumerate(local_maxima):\n",
    "            if m > threshold:\n",
    "                filtered_local_maxima.append(m)\n",
    "                filtered_local_maxima_indices.append(local_maxima_indices[i])\n",
    "\n",
    "        local_maxima = filtered_local_maxima\n",
    "        local_maxima_indices = filtered_local_maxima_indices\n",
    "\n",
    "    # print(\"LOCAL_MAXIMA_INDICES:\")\n",
    "    # print(list(local_maxima_indices))\n",
    "\n",
    "    return local_maxima_indices\n",
    "\n",
    "\n",
    "def get_timeseries(caption_indexes, features):\n",
    "    timeseries = []\n",
    "    for caption_index in caption_indexes:\n",
    "        timeseries.append(features[caption_index])\n",
    "    return timeseries\n",
    "\n",
    "\n",
    "def flatten_features(batches_features):\n",
    "    res = []\n",
    "    for batch_features in batches_features:\n",
    "        res += batch_features\n",
    "    return res\n",
    "\n",
    "\n",
    "def split_list(a, n):\n",
    "    k, m = divmod(len(a), n)\n",
    "    return (\n",
    "        a[i * k + min(i, m): (i + 1) * k + min(i + 1, m)]\n",
    "        for i in range(min(len(a), n))\n",
    "    )\n",
    "\n",
    "\n",
    "def topic_segmentation(\n",
    "        topic_segmentation_algorithm: TopicSegmentationAlgorithm,\n",
    "        df: pd.DataFrame,\n",
    "        meeting_id_col_name: str,\n",
    "        start_col_name: str,\n",
    "        end_col_name: str,\n",
    "        caption_col_name: str,\n",
    "        topic_segmentation_config: TopicSegmentationConfig,\n",
    "):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        df: dataframe with meeting captions\n",
    "    Output:\n",
    "        {meeting_id: [list of topic change indexes]}\n",
    "    \"\"\"\n",
    "\n",
    "    if topic_segmentation_algorithm == TopicSegmentationAlgorithm.BERT:\n",
    "        return topic_segmentation_bert(\n",
    "            df,\n",
    "            meeting_id_col_name,\n",
    "            start_col_name,\n",
    "            end_col_name,\n",
    "            caption_col_name,\n",
    "            topic_segmentation_config,\n",
    "        )\n",
    "    elif topic_segmentation_algorithm == TopicSegmentationAlgorithm.RANDOM:\n",
    "        return topic_segmentation_random(\n",
    "            df, meeting_id_col_name, start_col_name, end_col_name, caption_col_name\n",
    "        )\n",
    "    elif topic_segmentation_algorithm == TopicSegmentationAlgorithm.EVEN:\n",
    "        return topic_segmentation_even(\n",
    "            df, meeting_id_col_name, start_col_name, end_col_name, caption_col_name\n",
    "        )\n",
    "    else:\n",
    "        return topic_segmentation_bert(\n",
    "            df,\n",
    "            meeting_id_col_name,\n",
    "            start_col_name,\n",
    "            end_col_name,\n",
    "            caption_col_name,\n",
    "            topic_segmentation_config,\n",
    "        )\n",
    "\n",
    "\n",
    "default_meeting_id_column = 'meeting_id'\n",
    "default_st_column = 'st'\n",
    "default_en_column = 'en'\n",
    "default_caption_column = 'caption'\n",
    "\n",
    "\n",
    "def topic_segmentation_bert(\n",
    "        df: pd.DataFrame,\n",
    "        meeting_id_col_name: str = default_meeting_id_column,\n",
    "        start_col_name: str = default_st_column,\n",
    "        end_col_name: str = default_en_column,\n",
    "        caption_col_name: str = default_caption_column,\n",
    "        topic_segmentation_configs: TopicSegmentationConfig = TopicSegmentationConfig(),\n",
    "):\n",
    "    textiling_hyperparameters = topic_segmentation_configs.TEXT_TILING\n",
    "\n",
    "    # parallel inference\n",
    "    features = get_features_from_sentence(df[caption_col_name])\n",
    "\n",
    "    # meeting_id -> list of topic change start times\n",
    "    segments = {}\n",
    "    task_idx = 0\n",
    "    print(\"meeting_id -> task_idx\")\n",
    "    for meeting_id in set(df[meeting_id_col_name]):\n",
    "        print(\"%s -> %d\" % (meeting_id, task_idx))\n",
    "        task_idx += 1\n",
    "\n",
    "        meeting_data = df[df[meeting_id_col_name] == meeting_id]\n",
    "        caption_indexes = list(meeting_data.index)\n",
    "\n",
    "        timeseries = get_timeseries(caption_indexes, features)\n",
    "        block_comparison_score_timeseries = block_comparison_score(\n",
    "            timeseries, k=textiling_hyperparameters.SENTENCE_COMPARISON_WINDOW\n",
    "        )\n",
    "\n",
    "        block_comparison_score_timeseries = smooth(\n",
    "            block_comparison_score_timeseries,\n",
    "            n=textiling_hyperparameters.SMOOTHING_PASSES,\n",
    "            s=textiling_hyperparameters.SMOOTHING_WINDOW,\n",
    "        )\n",
    "\n",
    "        depth_score_timeseries = depth_score(block_comparison_score_timeseries)\n",
    "\n",
    "        meeting_start_time = meeting_data[start_col_name].iloc[0]\n",
    "        meeting_end_time = meeting_data[end_col_name].iloc[-1]\n",
    "        meeting_duration = meeting_end_time - meeting_start_time\n",
    "        segments[meeting_id] = depth_score_to_topic_change_indexes(\n",
    "            depth_score_timeseries,\n",
    "            meeting_duration,\n",
    "            topic_segmentation_configs=topic_segmentation_configs,\n",
    "        )\n",
    "\n",
    "    return segments\n",
    "\n",
    "\n",
    "from random import random\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def topic_segmentation_random(\n",
    "        df: pd.DataFrame,\n",
    "        meeting_id_col_name: str,\n",
    "        start_col_name: str,\n",
    "        end_col_name: str,\n",
    "        caption_col_name: str,\n",
    "        random_threshold: float = 0.9,\n",
    "):\n",
    "    # meeting_id -> list of topic change start times\n",
    "    segments = {}\n",
    "    task_idx = 0\n",
    "    print(\"meeting_id -> task_idx\")\n",
    "    for meeting_id in set(df[meeting_id_col_name]):\n",
    "        print(\"%s -> %d\" % (meeting_id, task_idx))\n",
    "        task_idx += 1\n",
    "\n",
    "        meeting_data = df[df[meeting_id_col_name] == meeting_id]\n",
    "        meeting_start_times = meeting_data[start_col_name]\n",
    "        random_segmentation = []\n",
    "        for i, _ in enumerate(meeting_start_times):\n",
    "            if random() > random_threshold:\n",
    "                random_segmentation.append(i)\n",
    "        print(random_segmentation)\n",
    "        segments[meeting_id] = random_segmentation\n",
    "    return segments\n",
    "\n",
    "\n",
    "def topic_segmentation_even(\n",
    "        df: pd.DataFrame,\n",
    "        meeting_id_col_name: str,\n",
    "        start_col_name: str,\n",
    "        end_col_name: str,\n",
    "        caption_col_name: str,\n",
    "):\n",
    "    # meeting_id -> list of topic change start times\n",
    "    segments = {}\n",
    "    task_idx = 0\n",
    "    print(\"meeting_id -> task_idx\")\n",
    "    for meeting_id in set(df[meeting_id_col_name]):\n",
    "        print(\"%s -> %d\" % (meeting_id, task_idx))\n",
    "        task_idx += 1\n",
    "\n",
    "        meeting_data = df[df[meeting_id_col_name] == meeting_id]\n",
    "        meeting_start_times = meeting_data[start_col_name]\n",
    "        even_segmentation = []\n",
    "        for i, _ in enumerate(meeting_start_times):\n",
    "            if i % 30 == 0:\n",
    "                even_segmentation.append(i)\n",
    "        print(even_segmentation)\n",
    "        segments[meeting_id] = even_segmentation\n",
    "    return segments\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import logging\n",
    "from bisect import bisect\n",
    "from typing import Dict\n",
    "import pandas as pd\n",
    "from nltk.metrics.segmentation import pk, windowdiff\n",
    "\n",
    "\n",
    "def compute_metrics(prediction_segmentations, binary_labels, metric_name_suffix=\"\"):\n",
    "    print(prediction_segmentations)\n",
    "    indices = {k: [i for i, v in enumerate(binary_labels[k]) if v == 1] for k in binary_labels.keys()}\n",
    "    print(f'expected: {indices}')\n",
    "    _pk, _windiff = [], []\n",
    "    for meeting_id, reference_segmentation in binary_labels.items():\n",
    "\n",
    "        predicted_segmentation_indexes = prediction_segmentations[meeting_id]\n",
    "        # we need to convert from topic changes indexes to topic changes binaries\n",
    "        predicted_segmentation = [0] * len(reference_segmentation)\n",
    "        for topic_change_index in predicted_segmentation_indexes:\n",
    "            predicted_segmentation[topic_change_index] = 1\n",
    "\n",
    "        reference_segmentation = \"\".join(map(str, reference_segmentation))\n",
    "        predicted_segmentation = \"\".join(map(str, predicted_segmentation))\n",
    "\n",
    "        _pk.append(pk(reference_segmentation, predicted_segmentation))\n",
    "\n",
    "        # setting k to default value used in CoAP (pk) function for both evaluation functions\n",
    "        k = int(\n",
    "            round(\n",
    "                len(reference_segmentation) / (reference_segmentation.count(\"1\") * 2.0)\n",
    "            )\n",
    "        )\n",
    "        _windiff.append(windowdiff(reference_segmentation, predicted_segmentation, k))\n",
    "\n",
    "    avg_pk = sum(_pk) / len(binary_labels)\n",
    "    avg_windiff = sum(_windiff) / len(binary_labels)\n",
    "\n",
    "    print(\"Pk on {} meetings: {}\".format(len(binary_labels), avg_pk))\n",
    "    print(\"WinDiff on {} meetings: {}\".format(len(binary_labels), avg_windiff))\n",
    "\n",
    "    return {\n",
    "        \"average_Pk_\" + str(metric_name_suffix): avg_pk,\n",
    "        \"average_windiff_\" + str(metric_name_suffix): avg_windiff,\n",
    "    }\n",
    "\n",
    "\n",
    "def binary_labels_flattened(\n",
    "        input_df,\n",
    "        labels_df,\n",
    "        meeting_id_col_name: str,\n",
    "        start_col_name: str,\n",
    "        end_col_name: str,\n",
    "        caption_col_name: str,\n",
    "):\n",
    "    \"\"\"\n",
    "    Binary Label [0, 0, 1, 0] for topic changes as ntlk format.\n",
    "    Hierarchical topic strutcure flattened.\n",
    "    see https://www.XXXX.com/intern/anp/view/?id=434543\n",
    "    \"\"\"\n",
    "    labels_flattened = {}\n",
    "    meeting_ids = list(set(input_df[meeting_id_col_name]))\n",
    "\n",
    "    for meeting_id in meeting_ids:\n",
    "        logging.info(\"\\n\\nMEETING ID:{}\".format(meeting_id))\n",
    "\n",
    "        if meeting_id not in list(labels_df[meeting_id_col_name]):\n",
    "            logging.info(\"{} not found in `labels_df`\".format(meeting_id))\n",
    "            continue\n",
    "\n",
    "        meeting_data = input_df[\n",
    "            input_df[meeting_id_col_name] == meeting_id\n",
    "            ].sort_values(by=[start_col_name])\n",
    "        meeting_sentences = [*map(lambda s: s.lower(), list(meeting_data[\"caption\"]))]\n",
    "\n",
    "        caption_start_times = list(meeting_data[start_col_name])\n",
    "        segment_start_times = list(\n",
    "            labels_df[labels_df[meeting_id_col_name] == meeting_id][start_col_name]\n",
    "        )\n",
    "\n",
    "        meeting_labels_flattened = [0] * len(caption_start_times)\n",
    "\n",
    "        # we skip first and last labaled segment cause they are naive segments\n",
    "        for sst in segment_start_times[1:]:\n",
    "            try:\n",
    "                topic_change_index = caption_start_times.index(sst)\n",
    "            except ValueError:\n",
    "                topic_change_index = bisect(caption_start_times, sst)\n",
    "                if topic_change_index == len(meeting_labels_flattened):\n",
    "                    topic_change_index -= 1  # bisect my go out of boundary\n",
    "            meeting_labels_flattened[topic_change_index] = 1\n",
    "\n",
    "        labels_flattened[meeting_id] = meeting_labels_flattened\n",
    "\n",
    "        logging.info(\"MEETING TRANSCRIPTS\")\n",
    "        for i, sentence in enumerate(meeting_sentences):\n",
    "            if meeting_labels_flattened[i] == 1:\n",
    "                logging.info(\"\\n\\n<<------ Topic Change () ------>>\\n\")\n",
    "            logging.info(sentence)\n",
    "\n",
    "    return labels_flattened\n",
    "\n",
    "\n",
    "def binary_labels_top_level(\n",
    "        input_df,\n",
    "        labels_df,\n",
    "        meeting_id_col_name: str,\n",
    "        start_col_name: str,\n",
    "        end_col_name: str,\n",
    "        caption_col_name: str,\n",
    "):\n",
    "    \"\"\"\n",
    "    Binary Label [0, 0, 1, 0] for topic changes as ntlk format.\n",
    "    Hierarchical topic strutcure only top level topics\n",
    "    see https://www.XXXX.com/intern/anp/view/?id=434543\n",
    "    \"\"\"\n",
    "    labels_top_level = {}\n",
    "    meeting_ids = list(set(input_df[meeting_id_col_name]))\n",
    "\n",
    "    for meeting_id in meeting_ids:\n",
    "        logging.info(\"\\n\\nMEETING ID:{}\".format(meeting_id))\n",
    "\n",
    "        if meeting_id not in list(labels_df[meeting_id_col_name]):\n",
    "            logging.info(\"{} not found in `labels_df`\".format(meeting_id))\n",
    "            continue\n",
    "\n",
    "        meeting_data = input_df[\n",
    "            input_df[meeting_id_col_name] == meeting_id\n",
    "            ].sort_values(by=[start_col_name])\n",
    "        meeting_sentences = [*map(lambda s: s.lower(), list(meeting_data[\"caption\"]))]\n",
    "\n",
    "        caption_start_times = list(meeting_data[start_col_name])\n",
    "        segment_start_times = list(\n",
    "            labels_df[labels_df[meeting_id_col_name] == meeting_id][start_col_name]\n",
    "        )\n",
    "        segment_end_times = list(\n",
    "            labels_df[labels_df[meeting_id_col_name] == meeting_id][end_col_name]\n",
    "        )\n",
    "\n",
    "        meeting_labels_top_level = [0] * len(caption_start_times)\n",
    "\n",
    "        high_level_topics_indexes = []\n",
    "        i = 0\n",
    "        while i < len(segment_end_times):\n",
    "            end = segment_end_times[i]\n",
    "            high_level_topics_indexes.append(i)\n",
    "            if segment_end_times.count(end) == 2:\n",
    "                # skip all the subtopics of this high level topic\n",
    "                i = (\n",
    "                        segment_end_times.index(end)\n",
    "                        + segment_end_times[segment_end_times.index(end) + 1:].index(end)\n",
    "                        + 2\n",
    "                )\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "        segment_start_times_high_level = [\n",
    "            segment_start_times[i] for i in high_level_topics_indexes\n",
    "        ]\n",
    "\n",
    "        # we skip first and last labaled segment cause they are naive segments\n",
    "        for sst in segment_start_times_high_level[1:]:\n",
    "            try:\n",
    "                topic_change_index = caption_start_times.index(sst)\n",
    "            except ValueError:\n",
    "                topic_change_index = bisect(caption_start_times, sst)\n",
    "                if topic_change_index == len(meeting_labels_top_level):\n",
    "                    topic_change_index -= 1  # bisect my go out of boundary\n",
    "            meeting_labels_top_level[topic_change_index] = 1\n",
    "\n",
    "        labels_top_level[meeting_id] = meeting_labels_top_level\n",
    "\n",
    "        logging.info(\"MEETING TRANSCRIPTS\")\n",
    "        for i, sentence in enumerate(meeting_sentences):\n",
    "            if meeting_labels_top_level[i] == 1:\n",
    "                logging.info(\"\\n\\n<<------ Topic Change () ------>>\\n\")\n",
    "            logging.info(sentence)\n",
    "\n",
    "    return labels_top_level\n",
    "\n",
    "\n",
    "MEETING_ID_COL_NAME = \"meeting_id\"\n",
    "START_COL_NAME = \"st\"\n",
    "EN_COL_NAME = \"en\"\n",
    "CAPTION_COL_NAME = \"caption\"\n",
    "\n",
    "\n",
    "def eval_topic_segmentation(\n",
    "        input_df: pd.DataFrame,\n",
    "        label_df: pd.DataFrame,\n",
    "        topic_segmentation_algorithm: TopicSegmentationAlgorithm,\n",
    "        topic_segmentation_config: TopicSegmentationConfig,\n",
    ") -> Dict[str, float]:\n",
    "    prediction_segmentations = topic_segmentation(\n",
    "        topic_segmentation_algorithm,\n",
    "        input_df,\n",
    "        MEETING_ID_COL_NAME,\n",
    "        START_COL_NAME,\n",
    "        EN_COL_NAME,\n",
    "        CAPTION_COL_NAME,\n",
    "        topic_segmentation_config,\n",
    "    )\n",
    "\n",
    "    flattened = binary_labels_flattened(\n",
    "        input_df,\n",
    "        label_df,\n",
    "        MEETING_ID_COL_NAME,\n",
    "        START_COL_NAME,\n",
    "        EN_COL_NAME,\n",
    "        CAPTION_COL_NAME,\n",
    "    )\n",
    "\n",
    "    top_level = binary_labels_top_level(\n",
    "        input_df,\n",
    "        label_df,\n",
    "        MEETING_ID_COL_NAME,\n",
    "        START_COL_NAME,\n",
    "        EN_COL_NAME,\n",
    "        CAPTION_COL_NAME,\n",
    "    )\n",
    "\n",
    "    flattened_metrics = compute_metrics(\n",
    "        prediction_segmentations, flattened, metric_name_suffix=\"flattened\"\n",
    "    )\n",
    "    top_level_metrics = compute_metrics(\n",
    "        prediction_segmentations, top_level, metric_name_suffix=\"top_level\"\n",
    "    )\n",
    "\n",
    "    def merge_metrics(*metrics):\n",
    "        res = {}\n",
    "        for m in metrics:\n",
    "            for k, v in m.items():\n",
    "                res[k] = v\n",
    "        return res\n",
    "\n",
    "    return merge_metrics(flattened_metrics, top_level_metrics)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def preprocessing(df, caption_col_name):\n",
    "    fillers = [\"um\", \"uh\", \"oh\", \"hmm\", \"you know\", \"like\"]\n",
    "    fillers += list(\n",
    "        map(lambda filler: filler + \" \", fillers)\n",
    "    )  # filler inside caption with other words\n",
    "    fillers = list(\n",
    "        map(lambda filler: \"(?i)\" + filler, fillers)\n",
    "    )  # make it case-insensitive\n",
    "    df[caption_col_name].replace(fillers, [\"\"] * len(fillers), regex=True, inplace=True)\n",
    "\n",
    "    captions_with_multiple_setences = len(df.loc[df[caption_col_name].isin([\".\"])])\n",
    "    if captions_with_multiple_setences > 0:\n",
    "        print(\n",
    "            f\"WARNING: Found {captions_with_multiple_setences} captions with multiple sentences; sentence embeddings may be inaccurate.\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "\n",
    "    df = df[df[caption_col_name].str.len() > 20]\n",
    "    df.reset_index(inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def icsi_dataset():\n",
    "    pass\n",
    "\n",
    "\n",
    "def ami_dataset():\n",
    "    \"\"\"See XXXX for label generation and XXXX for input analysis\n",
    "\n",
    "            SELECT\n",
    "                fb_meeting_id AS meeting_id,\n",
    "                st,\n",
    "                en,\n",
    "                caption,\n",
    "                speaker\n",
    "            FROM {ami}\n",
    "            WHERE ds = '2021-01-12'\n",
    "\n",
    "            SELECT\n",
    "                fb_meeting_id AS meeting_id,\n",
    "                st,\n",
    "                en,\n",
    "                topic\n",
    "            FROM {labels}\n",
    "            WHERE ds = '2021-01-10'\n",
    "    \"\"\"\n",
    "    train = pd.read_csv('data/train_ami.csv')\n",
    "    test = pd.read_csv('data/test_ami.csv')\n",
    "    # train = train[train['meeting_id'] < 1]\n",
    "    # test = test[test['meeting_id'] < 1]\n",
    "    train = preprocessing(train, 'caption')\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def measure(f):\n",
    "    start_time = time.perf_counter()\n",
    "    f()\n",
    "    end_time = time.perf_counter()\n",
    "\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"Elapsed time:\", elapsed_time)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# def test_ami():\n",
    "#     input_df, label_df = ami_dataset()\n",
    "#     return eval_topic_segmentation(\n",
    "#         input_df,\n",
    "#         label_df,\n",
    "#         TopicSegmentationAlgorithm.SBERT,\n",
    "#         TopicSegmentationConfig(TextTilingHyperparameters()),\n",
    "#     )\n",
    "#\n",
    "# measure(test_ami)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nPk on 20 meetings: 0.42860887474411713\\nWinDiff on 20 meetings: 0.591437437455864\\n'"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Pk on 20 meetings: 0.42860887474411713\n",
    "WinDiff on 20 meetings: 0.591437437455864\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def convert_time_to_seconds(time_str):\n",
    "    time_parts = time_str.split(':')\n",
    "    hours = 0\n",
    "    minutes = 0\n",
    "    seconds = 0\n",
    "    milliseconds = 0\n",
    "\n",
    "    if len(time_parts) == 2:\n",
    "        minutes, seconds = map(float, time_parts)\n",
    "    elif len(time_parts) == 3:\n",
    "        hours, minutes, seconds = map(float, time_parts)\n",
    "\n",
    "    seconds += (hours * 3600) + (minutes * 60)\n",
    "\n",
    "    if '.' in str(seconds):\n",
    "        seconds, milliseconds = map(int, str(seconds).split('.'))\n",
    "\n",
    "    return (seconds + milliseconds/1000)\n",
    "\n",
    "\n",
    "def vvt_to_df(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = f.read()\n",
    "    meeting_id = file_path  # assuming meeting ID is in the file name and just an integer\n",
    "    utterances = re.findall(r'(\\d+:\\d+.\\d+).+?(\\d+:\\d+.\\d+)\\n(.*?)\\n\\n', data, re.DOTALL)\n",
    "    result = []\n",
    "    utterance_id = 1\n",
    "    for utterance in utterances:\n",
    "        st, en, caption = utterance\n",
    "        st_sec = convert_time_to_seconds(st)\n",
    "        en_sec = convert_time_to_seconds(en)\n",
    "        result.append(\n",
    "            {'meeting_id': meeting_id, 'st': st_sec, 'en': en_sec, 'caption': caption, 'speaker': utterance_id})\n",
    "        utterance_id += 1\n",
    "    return pd.DataFrame(result)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     meeting_id      st      en  \\\n",
      "0  ./data/episode_001_large.vtt   0.000   4.002   \n",
      "1  ./data/episode_001_large.vtt   4.002   6.006   \n",
      "2  ./data/episode_001_large.vtt   6.006   8.068   \n",
      "3  ./data/episode_001_large.vtt   8.068  11.092   \n",
      "4  ./data/episode_001_large.vtt  11.092  16.096   \n",
      "\n",
      "                                             caption  speaker  \n",
      "0   As part of MIT course 6S099, Artificial Gener...        1  \n",
      "1   I've gotten the chance to sit down with Max T...        2  \n",
      "2                     He is a professor here at MIT.        3  \n",
      "3   He's a physicist, spent a large part of his c...        4  \n",
      "4   studying the mysteries of our cosmological un...        5  \n",
      "meeting_id -> task_idx\n",
      "./data/episode_001_large.vtt -> 0\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'./data/episode_001_large.vtt': array([1623, 1603, 1552, 1514, 1415, 1385, 1162,  804,  569,  416,  381,\n         339,   33,   16])}"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = './data/episode_001_large.vtt'\n",
    "df = vvt_to_df(file_path)\n",
    "print(df.head())\n",
    "segments = topic_segmentation_bert(\n",
    "    df=df,\n",
    "    topic_segmentation_configs=TopicSegmentationConfig(TextTilingHyperparameters(TOPIC_CHANGE_THRESHOLD=0.5))\n",
    ")\n",
    "segments"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "                     meeting_id  start_index  end_index summary   start_time  \\\n0  ./data/episode_001_large.vtt            0         16          0:00:00.000   \n1  ./data/episode_001_large.vtt           16         33          0:00:47.097   \n2  ./data/episode_001_large.vtt           33        339          0:01:36.016   \n3  ./data/episode_001_large.vtt          339        381          0:15:34.056   \n4  ./data/episode_001_large.vtt          381        416          0:17:19.016   \n\n      end_time                                              topic  \n0  0:00:47.097   As part of MIT course 6S099, Artificial Gener...  \n1  0:01:36.016   please subscribe and also click the little be...  \n2  0:15:34.056   radio frequency interference, RFI, look it up...  \n3  0:17:19.016   or would you prefer that it's actually\\n not ...  \n4  0:18:47.011   oh, these animals can't feel pain.\\n It's oka...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>meeting_id</th>\n      <th>start_index</th>\n      <th>end_index</th>\n      <th>summary</th>\n      <th>start_time</th>\n      <th>end_time</th>\n      <th>topic</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>./data/episode_001_large.vtt</td>\n      <td>0</td>\n      <td>16</td>\n      <td></td>\n      <td>0:00:00.000</td>\n      <td>0:00:47.097</td>\n      <td>As part of MIT course 6S099, Artificial Gener...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>./data/episode_001_large.vtt</td>\n      <td>16</td>\n      <td>33</td>\n      <td></td>\n      <td>0:00:47.097</td>\n      <td>0:01:36.016</td>\n      <td>please subscribe and also click the little be...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>./data/episode_001_large.vtt</td>\n      <td>33</td>\n      <td>339</td>\n      <td></td>\n      <td>0:01:36.016</td>\n      <td>0:15:34.056</td>\n      <td>radio frequency interference, RFI, look it up...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>./data/episode_001_large.vtt</td>\n      <td>339</td>\n      <td>381</td>\n      <td></td>\n      <td>0:15:34.056</td>\n      <td>0:17:19.016</td>\n      <td>or would you prefer that it's actually\\n not ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>./data/episode_001_large.vtt</td>\n      <td>381</td>\n      <td>416</td>\n      <td></td>\n      <td>0:17:19.016</td>\n      <td>0:18:47.011</td>\n      <td>oh, these animals can't feel pain.\\n It's oka...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\", max_length=2048, truncation=True)\n",
    "\n",
    "# Assuming the list of indices is stored in a variable named topic_indices\n",
    "topic_indices = segments\n",
    "\n",
    "# Create an empty list to store the topics\n",
    "topics = []\n",
    "\n",
    "def convert_seconds_to_time(seconds):\n",
    "    hours = int(seconds // 3600)\n",
    "    seconds %= 3600\n",
    "    minutes = int(seconds // 60)\n",
    "    seconds %= 60\n",
    "    milliseconds = int((seconds % 1) * 1000)\n",
    "    seconds = int(seconds)\n",
    "    return f\"{hours}:{minutes:02}:{seconds:02}.{milliseconds:03}\"\n",
    "\n",
    "# Iterate over each file and its corresponding indices\n",
    "for file_path, indices in topic_indices.items():\n",
    "\n",
    "    # Sort the indices in ascending order\n",
    "    indices = sorted(indices)\n",
    "\n",
    "    # Add 0 as the starting index\n",
    "    indices = [0] + indices\n",
    "\n",
    "    # Iterate over each pair of indices\n",
    "    for i in range(len(indices) - 1):\n",
    "        # Extract the rows of the dataframe between the two indices\n",
    "        start_index = indices[i]\n",
    "        end_index = indices[i + 1]\n",
    "        df_subset = df[(df['meeting_id'] == file_path) & (df.index >= start_index) & (df.index < end_index)]\n",
    "\n",
    "        start_time = convert_seconds_to_time(df_subset.loc[start_index, 'st'])\n",
    "        end_time = convert_seconds_to_time(df_subset.loc[end_index - 1, 'en'])\n",
    "        # Combine the captions into a single string separated by a newline character\n",
    "        topic_caption = '\\n'.join(df_subset['caption'].tolist())\n",
    "        summary = summarizer(topic_caption, max_length=30, min_length=1, do_sample=False)[0]['summary_text']\n",
    "        # summary = \"\"\n",
    "\n",
    "        # Append the topic to the list of topics\n",
    "        topics.append({'meeting_id': file_path,\n",
    "                       'start_index': start_index,\n",
    "                       'end_index': end_index,\n",
    "                       'summary': summary,\n",
    "                       'start_time': start_time,\n",
    "                       'end_time': end_time,\n",
    "                       'topic': topic_caption})\n",
    "\n",
    "# Create a new dataframe with the topics\n",
    "topics_df = pd.DataFrame(topics)\n",
    "topics_df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "topics_df.to_csv('./output_topics.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
