{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nThis notebook uses approach from paper:\\n'"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This notebook uses approach from paper: https://arxiv.org/abs/2106.12978\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "PARALLEL_INFERENCE_INSTANCES = 20\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "from enum import Enum\n",
    "from typing import NamedTuple, Optional\n",
    "\n",
    "\n",
    "class TopicSegmentationAlgorithm(Enum):\n",
    "    RANDOM = 0\n",
    "    EVEN = 1\n",
    "    BERT = 2\n",
    "    SBERT = 3\n",
    "\n",
    "\n",
    "class TextTilingHyperparameters(NamedTuple):\n",
    "    SENTENCE_COMPARISON_WINDOW: int = 15\n",
    "    SMOOTHING_PASSES: int = 2\n",
    "    SMOOTHING_WINDOW: int = 1\n",
    "    TOPIC_CHANGE_THRESHOLD: float = 0.6\n",
    "\n",
    "\n",
    "class TopicSegmentationConfig(NamedTuple):\n",
    "    TEXT_TILING: Optional[TextTilingHyperparameters] = None\n",
    "    MAX_SEGMENTS_CAP: bool = True\n",
    "    MAX_SEGMENTS_CAP__AVERAGE_SEGMENT_LENGTH: int = 60\n",
    "\n",
    "\n",
    "def PrintMessage(msg, x):\n",
    "    print(msg)\n",
    "    print(x)\n",
    "\n",
    "\n",
    "def depth_score(timeseries):\n",
    "    \"\"\"\n",
    "    The depth score corresponds to how strongly the cues for a subtopic changed on both sides of a\n",
    "    given token-sequence gap and is based on the distance from the peaks on both sides of the valleyto that valley.\n",
    "\n",
    "    returns depth_scores\n",
    "    \"\"\"\n",
    "    depth_scores = []\n",
    "    for i in range(1, len(timeseries) - 1):\n",
    "        left, right = i - 1, i + 1\n",
    "        while left > 0 and timeseries[left - 1] > timeseries[left]:\n",
    "            left -= 1\n",
    "        while (\n",
    "                right < (len(timeseries) - 1) and timeseries[right + 1] > timeseries[right]\n",
    "        ):\n",
    "            right += 1\n",
    "        depth_scores.append(\n",
    "            (timeseries[right] - timeseries[i]) + (timeseries[left] - timeseries[i])\n",
    "        )\n",
    "    return depth_scores\n",
    "\n",
    "\n",
    "def smooth(timeseries, n, s):\n",
    "    smoothed_timeseries = timeseries[:]\n",
    "    for _ in range(n):\n",
    "        for index in range(len(smoothed_timeseries)):\n",
    "            neighbours = smoothed_timeseries[\n",
    "                         max(0, index - s): min(len(timeseries) - 1, index + s)\n",
    "                         ]\n",
    "            smoothed_timeseries[index] = sum(neighbours) / len(neighbours)\n",
    "    return smoothed_timeseries\n",
    "\n",
    "\n",
    "def sentences_similarity(first_sentence_features, second_sentence_features) -> float:\n",
    "    \"\"\"\n",
    "    Given two senteneces embedding features compute cosine similarity\n",
    "    \"\"\"\n",
    "    similarity_metric = torch.nn.CosineSimilarity()\n",
    "    return float(similarity_metric(first_sentence_features, second_sentence_features))\n",
    "\n",
    "\n",
    "def compute_window(timeseries, start_index, end_index):\n",
    "    \"\"\"given start and end index of embedding, compute pooled window value\n",
    "\n",
    "    [window_size, 768] -> [1, 768]\n",
    "    \"\"\"\n",
    "    stack = torch.stack(timeseries[start_index:end_index])\n",
    "    stack = stack.unsqueeze(\n",
    "        0\n",
    "    )  # https://jbencook.com/adding-a-dimension-to-a-tensor-in-pytorch/\n",
    "    stack_size = end_index - start_index\n",
    "    pooling = torch.nn.MaxPool2d((stack_size - 1, 1))\n",
    "    return pooling(stack)\n",
    "\n",
    "\n",
    "def block_comparison_score(timeseries, k):\n",
    "    \"\"\"\n",
    "    comparison score for a gap (i)\n",
    "\n",
    "    cfr. docstring of block_comparison_score\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for i in range(k, len(timeseries) - k):\n",
    "        first_window_features = compute_window(timeseries, i - k, i + 1)\n",
    "        second_window_features = compute_window(timeseries, i + 1, i + k + 2)\n",
    "        res.append(\n",
    "            sentences_similarity(first_window_features[0], second_window_features[0])\n",
    "        )\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_features_from_sentence(batch_sentences, layer=-2):\n",
    "    \"\"\"\n",
    "    extracts the BERT semantic representation\n",
    "    from a sentence, using an averaged value of\n",
    "    the `layer`-th layer\n",
    "\n",
    "    returns a 1-dimensional tensor of size 758\n",
    "    \"\"\"\n",
    "\n",
    "    return model.encode(batch_sentences, convert_to_numpy=False)\n",
    "\n",
    "\n",
    "def arsort2(array1, array2):\n",
    "    x = np.array(array1)\n",
    "    y = np.array(array2)\n",
    "\n",
    "    sorted_idx = x.argsort()[::-1]\n",
    "    return x[sorted_idx], y[sorted_idx]\n",
    "\n",
    "\n",
    "def get_local_maxima(array):\n",
    "    local_maxima_indices = []\n",
    "    local_maxima_values = []\n",
    "    for i in range(1, len(array) - 1):\n",
    "        if array[i - 1] < array[i] and array[i] > array[i + 1]:\n",
    "            local_maxima_indices.append(i)\n",
    "            local_maxima_values.append(array[i])\n",
    "    return local_maxima_indices, local_maxima_values\n",
    "\n",
    "\n",
    "def depth_score_to_topic_change_indexes(\n",
    "        depth_score_timeseries,\n",
    "        meeting_duration=60 * 3600,\n",
    "        topic_segmentation_configs=TopicSegmentationConfig,\n",
    "):\n",
    "    \"\"\"\n",
    "    capped add a max segment limit so there are not too many segments, used for UI improvements on the Workplace TeamWork product\n",
    "    \"\"\"\n",
    "\n",
    "    capped = topic_segmentation_configs.MAX_SEGMENTS_CAP\n",
    "    average_segment_length = (\n",
    "        topic_segmentation_configs.MAX_SEGMENTS_CAP__AVERAGE_SEGMENT_LENGTH\n",
    "    )\n",
    "    threshold = topic_segmentation_configs.TEXT_TILING.TOPIC_CHANGE_THRESHOLD * max(\n",
    "        depth_score_timeseries\n",
    "    )\n",
    "\n",
    "    # print(\"DEPTH_SCORE_TIMESERIES:\")\n",
    "    # print(list(depth_score_timeseries))\n",
    "\n",
    "    if depth_score_timeseries == []:\n",
    "        return []\n",
    "\n",
    "    local_maxima_indices, local_maxima = get_local_maxima(depth_score_timeseries)\n",
    "\n",
    "    if local_maxima == []:\n",
    "        return []\n",
    "\n",
    "    if capped:  # capped is segmentation used for UI\n",
    "        # sort based on maxima for pruning\n",
    "        local_maxima, local_maxima_indices = arsort2(local_maxima, local_maxima_indices)\n",
    "\n",
    "        # local maxima are sorted by depth_score value and we take only the first K\n",
    "        # where the K+1th local maxima is lower then the threshold\n",
    "        for thres in range(len(local_maxima)):\n",
    "            if local_maxima[thres] <= threshold:\n",
    "                break\n",
    "\n",
    "        max_segments = int(meeting_duration / average_segment_length)\n",
    "        slice_length = min(max_segments, thres)\n",
    "\n",
    "        local_maxima_indices = local_maxima_indices[:slice_length]\n",
    "        local_maxima = local_maxima[:slice_length]\n",
    "\n",
    "        # after pruning, sort again based on indices for chronological ordering\n",
    "        local_maxima_indices, _ = arsort2(local_maxima_indices, local_maxima)\n",
    "\n",
    "    else:  # this is the vanilla TextTiling used for Pk optimization\n",
    "        filtered_local_maxima_indices = []\n",
    "        filtered_local_maxima = []\n",
    "\n",
    "        for i, m in enumerate(local_maxima):\n",
    "            if m > threshold:\n",
    "                filtered_local_maxima.append(m)\n",
    "                filtered_local_maxima_indices.append(local_maxima_indices[i])\n",
    "\n",
    "        local_maxima = filtered_local_maxima\n",
    "        local_maxima_indices = filtered_local_maxima_indices\n",
    "\n",
    "    # print(\"LOCAL_MAXIMA_INDICES:\")\n",
    "    # print(list(local_maxima_indices))\n",
    "\n",
    "    return local_maxima_indices\n",
    "\n",
    "\n",
    "def get_timeseries(caption_indexes, features):\n",
    "    timeseries = []\n",
    "    for caption_index in caption_indexes:\n",
    "        timeseries.append(features[caption_index])\n",
    "    return timeseries\n",
    "\n",
    "\n",
    "def flatten_features(batches_features):\n",
    "    res = []\n",
    "    for batch_features in batches_features:\n",
    "        res += batch_features\n",
    "    return res\n",
    "\n",
    "\n",
    "def split_list(a, n):\n",
    "    k, m = divmod(len(a), n)\n",
    "    return (\n",
    "        a[i * k + min(i, m): (i + 1) * k + min(i + 1, m)]\n",
    "        for i in range(min(len(a), n))\n",
    "    )\n",
    "\n",
    "\n",
    "def topic_segmentation(\n",
    "        topic_segmentation_algorithm: TopicSegmentationAlgorithm,\n",
    "        df: pd.DataFrame,\n",
    "        meeting_id_col_name: str,\n",
    "        start_col_name: str,\n",
    "        end_col_name: str,\n",
    "        caption_col_name: str,\n",
    "        topic_segmentation_config: TopicSegmentationConfig,\n",
    "):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        df: dataframe with meeting captions\n",
    "    Output:\n",
    "        {meeting_id: [list of topic change indexes]}\n",
    "    \"\"\"\n",
    "\n",
    "    if topic_segmentation_algorithm == TopicSegmentationAlgorithm.BERT:\n",
    "        return topic_segmentation_bert(\n",
    "            df,\n",
    "            meeting_id_col_name,\n",
    "            start_col_name,\n",
    "            end_col_name,\n",
    "            caption_col_name,\n",
    "            topic_segmentation_config,\n",
    "        )\n",
    "    elif topic_segmentation_algorithm == TopicSegmentationAlgorithm.RANDOM:\n",
    "        return topic_segmentation_random(\n",
    "            df, meeting_id_col_name, start_col_name, end_col_name, caption_col_name\n",
    "        )\n",
    "    elif topic_segmentation_algorithm == TopicSegmentationAlgorithm.EVEN:\n",
    "        return topic_segmentation_even(\n",
    "            df, meeting_id_col_name, start_col_name, end_col_name, caption_col_name\n",
    "        )\n",
    "    else:\n",
    "        return topic_segmentation_bert(\n",
    "            df,\n",
    "            meeting_id_col_name,\n",
    "            start_col_name,\n",
    "            end_col_name,\n",
    "            caption_col_name,\n",
    "            topic_segmentation_config,\n",
    "        )\n",
    "\n",
    "\n",
    "def topic_segmentation_bert(\n",
    "        df: pd.DataFrame,\n",
    "        meeting_id_col_name: str,\n",
    "        start_col_name: str,\n",
    "        end_col_name: str,\n",
    "        caption_col_name: str,\n",
    "        topic_segmentation_configs: TopicSegmentationConfig,\n",
    "):\n",
    "    textiling_hyperparameters = topic_segmentation_configs.TEXT_TILING\n",
    "\n",
    "    # parallel inference\n",
    "    features = get_features_from_sentence(df[caption_col_name])\n",
    "\n",
    "    # meeting_id -> list of topic change start times\n",
    "    segments = {}\n",
    "    task_idx = 0\n",
    "    print(\"meeting_id -> task_idx\")\n",
    "    for meeting_id in set(df[meeting_id_col_name]):\n",
    "        print(\"%s -> %d\" % (meeting_id, task_idx))\n",
    "        task_idx += 1\n",
    "\n",
    "        meeting_data = df[df[meeting_id_col_name] == meeting_id]\n",
    "        caption_indexes = list(meeting_data.index)\n",
    "\n",
    "        timeseries = get_timeseries(caption_indexes, features)\n",
    "        block_comparison_score_timeseries = block_comparison_score(\n",
    "            timeseries, k=textiling_hyperparameters.SENTENCE_COMPARISON_WINDOW\n",
    "        )\n",
    "\n",
    "        block_comparison_score_timeseries = smooth(\n",
    "            block_comparison_score_timeseries,\n",
    "            n=textiling_hyperparameters.SMOOTHING_PASSES,\n",
    "            s=textiling_hyperparameters.SMOOTHING_WINDOW,\n",
    "        )\n",
    "\n",
    "        depth_score_timeseries = depth_score(block_comparison_score_timeseries)\n",
    "\n",
    "        meeting_start_time = meeting_data[start_col_name].iloc[0]\n",
    "        meeting_end_time = meeting_data[end_col_name].iloc[-1]\n",
    "        meeting_duration = meeting_end_time - meeting_start_time\n",
    "        segments[meeting_id] = depth_score_to_topic_change_indexes(\n",
    "            depth_score_timeseries,\n",
    "            meeting_duration,\n",
    "            topic_segmentation_configs=topic_segmentation_configs,\n",
    "        )\n",
    "\n",
    "    return segments\n",
    "\n",
    "\n",
    "from random import random\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def topic_segmentation_random(\n",
    "        df: pd.DataFrame,\n",
    "        meeting_id_col_name: str,\n",
    "        start_col_name: str,\n",
    "        end_col_name: str,\n",
    "        caption_col_name: str,\n",
    "        random_threshold: float = 0.9,\n",
    "):\n",
    "    # meeting_id -> list of topic change start times\n",
    "    segments = {}\n",
    "    task_idx = 0\n",
    "    print(\"meeting_id -> task_idx\")\n",
    "    for meeting_id in set(df[meeting_id_col_name]):\n",
    "        print(\"%s -> %d\" % (meeting_id, task_idx))\n",
    "        task_idx += 1\n",
    "\n",
    "        meeting_data = df[df[meeting_id_col_name] == meeting_id]\n",
    "        meeting_start_times = meeting_data[start_col_name]\n",
    "        random_segmentation = []\n",
    "        for i, _ in enumerate(meeting_start_times):\n",
    "            if random() > random_threshold:\n",
    "                random_segmentation.append(i)\n",
    "        print(random_segmentation)\n",
    "        segments[meeting_id] = random_segmentation\n",
    "    return segments\n",
    "\n",
    "\n",
    "def topic_segmentation_even(\n",
    "        df: pd.DataFrame,\n",
    "        meeting_id_col_name: str,\n",
    "        start_col_name: str,\n",
    "        end_col_name: str,\n",
    "        caption_col_name: str,\n",
    "):\n",
    "    # meeting_id -> list of topic change start times\n",
    "    segments = {}\n",
    "    task_idx = 0\n",
    "    print(\"meeting_id -> task_idx\")\n",
    "    for meeting_id in set(df[meeting_id_col_name]):\n",
    "        print(\"%s -> %d\" % (meeting_id, task_idx))\n",
    "        task_idx += 1\n",
    "\n",
    "        meeting_data = df[df[meeting_id_col_name] == meeting_id]\n",
    "        meeting_start_times = meeting_data[start_col_name]\n",
    "        even_segmentation = []\n",
    "        for i, _ in enumerate(meeting_start_times):\n",
    "            if i % 30 == 0:\n",
    "                even_segmentation.append(i)\n",
    "        print(even_segmentation)\n",
    "        segments[meeting_id] = even_segmentation\n",
    "    return segments\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import logging\n",
    "from bisect import bisect\n",
    "from typing import Dict\n",
    "import pandas as pd\n",
    "from nltk.metrics.segmentation import pk, windowdiff\n",
    "\n",
    "\n",
    "def compute_metrics(prediction_segmentations, binary_labels, metric_name_suffix=\"\"):\n",
    "    print(prediction_segmentations)\n",
    "    indices = {k: [i for i, v in enumerate(binary_labels[k]) if v == 1] for k in binary_labels.keys() }\n",
    "    print(f'expected: {indices}')\n",
    "    _pk, _windiff = [], []\n",
    "    for meeting_id, reference_segmentation in binary_labels.items():\n",
    "\n",
    "        predicted_segmentation_indexes = prediction_segmentations[meeting_id]\n",
    "        # we need to convert from topic changes indexes to topic changes binaries\n",
    "        predicted_segmentation = [0] * len(reference_segmentation)\n",
    "        for topic_change_index in predicted_segmentation_indexes:\n",
    "            predicted_segmentation[topic_change_index] = 1\n",
    "\n",
    "        reference_segmentation = \"\".join(map(str, reference_segmentation))\n",
    "        predicted_segmentation = \"\".join(map(str, predicted_segmentation))\n",
    "\n",
    "        _pk.append(pk(reference_segmentation, predicted_segmentation))\n",
    "\n",
    "        # setting k to default value used in CoAP (pk) function for both evaluation functions\n",
    "        k = int(\n",
    "            round(\n",
    "                len(reference_segmentation) / (reference_segmentation.count(\"1\") * 2.0)\n",
    "            )\n",
    "        )\n",
    "        _windiff.append(windowdiff(reference_segmentation, predicted_segmentation, k))\n",
    "\n",
    "    avg_pk = sum(_pk) / len(binary_labels)\n",
    "    avg_windiff = sum(_windiff) / len(binary_labels)\n",
    "\n",
    "    print(\"Pk on {} meetings: {}\".format(len(binary_labels), avg_pk))\n",
    "    print(\"WinDiff on {} meetings: {}\".format(len(binary_labels), avg_windiff))\n",
    "\n",
    "    return {\n",
    "        \"average_Pk_\" + str(metric_name_suffix): avg_pk,\n",
    "        \"average_windiff_\" + str(metric_name_suffix): avg_windiff,\n",
    "    }\n",
    "\n",
    "\n",
    "def binary_labels_flattened(\n",
    "        input_df,\n",
    "        labels_df,\n",
    "        meeting_id_col_name: str,\n",
    "        start_col_name: str,\n",
    "        end_col_name: str,\n",
    "        caption_col_name: str,\n",
    "):\n",
    "    \"\"\"\n",
    "    Binary Label [0, 0, 1, 0] for topic changes as ntlk format.\n",
    "    Hierarchical topic strutcure flattened.\n",
    "    see https://www.XXXX.com/intern/anp/view/?id=434543\n",
    "    \"\"\"\n",
    "    labels_flattened = {}\n",
    "    meeting_ids = list(set(input_df[meeting_id_col_name]))\n",
    "\n",
    "    for meeting_id in meeting_ids:\n",
    "        logging.info(\"\\n\\nMEETING ID:{}\".format(meeting_id))\n",
    "\n",
    "        if meeting_id not in list(labels_df[meeting_id_col_name]):\n",
    "            logging.info(\"{} not found in `labels_df`\".format(meeting_id))\n",
    "            continue\n",
    "\n",
    "        meeting_data = input_df[\n",
    "            input_df[meeting_id_col_name] == meeting_id\n",
    "            ].sort_values(by=[start_col_name])\n",
    "        meeting_sentences = [*map(lambda s: s.lower(), list(meeting_data[\"caption\"]))]\n",
    "\n",
    "        caption_start_times = list(meeting_data[start_col_name])\n",
    "        segment_start_times = list(\n",
    "            labels_df[labels_df[meeting_id_col_name] == meeting_id][start_col_name]\n",
    "        )\n",
    "\n",
    "        meeting_labels_flattened = [0] * len(caption_start_times)\n",
    "\n",
    "        # we skip first and last labaled segment cause they are naive segments\n",
    "        for sst in segment_start_times[1:]:\n",
    "            try:\n",
    "                topic_change_index = caption_start_times.index(sst)\n",
    "            except ValueError:\n",
    "                topic_change_index = bisect(caption_start_times, sst)\n",
    "                if topic_change_index == len(meeting_labels_flattened):\n",
    "                    topic_change_index -= 1  # bisect my go out of boundary\n",
    "            meeting_labels_flattened[topic_change_index] = 1\n",
    "\n",
    "        labels_flattened[meeting_id] = meeting_labels_flattened\n",
    "\n",
    "        logging.info(\"MEETING TRANSCRIPTS\")\n",
    "        for i, sentence in enumerate(meeting_sentences):\n",
    "            if meeting_labels_flattened[i] == 1:\n",
    "                logging.info(\"\\n\\n<<------ Topic Change () ------>>\\n\")\n",
    "            logging.info(sentence)\n",
    "\n",
    "    return labels_flattened\n",
    "\n",
    "\n",
    "def binary_labels_top_level(\n",
    "        input_df,\n",
    "        labels_df,\n",
    "        meeting_id_col_name: str,\n",
    "        start_col_name: str,\n",
    "        end_col_name: str,\n",
    "        caption_col_name: str,\n",
    "):\n",
    "    \"\"\"\n",
    "    Binary Label [0, 0, 1, 0] for topic changes as ntlk format.\n",
    "    Hierarchical topic strutcure only top level topics\n",
    "    see https://www.XXXX.com/intern/anp/view/?id=434543\n",
    "    \"\"\"\n",
    "    labels_top_level = {}\n",
    "    meeting_ids = list(set(input_df[meeting_id_col_name]))\n",
    "\n",
    "    for meeting_id in meeting_ids:\n",
    "        logging.info(\"\\n\\nMEETING ID:{}\".format(meeting_id))\n",
    "\n",
    "        if meeting_id not in list(labels_df[meeting_id_col_name]):\n",
    "            logging.info(\"{} not found in `labels_df`\".format(meeting_id))\n",
    "            continue\n",
    "\n",
    "        meeting_data = input_df[\n",
    "            input_df[meeting_id_col_name] == meeting_id\n",
    "            ].sort_values(by=[start_col_name])\n",
    "        meeting_sentences = [*map(lambda s: s.lower(), list(meeting_data[\"caption\"]))]\n",
    "\n",
    "        caption_start_times = list(meeting_data[start_col_name])\n",
    "        segment_start_times = list(\n",
    "            labels_df[labels_df[meeting_id_col_name] == meeting_id][start_col_name]\n",
    "        )\n",
    "        segment_end_times = list(\n",
    "            labels_df[labels_df[meeting_id_col_name] == meeting_id][end_col_name]\n",
    "        )\n",
    "\n",
    "        meeting_labels_top_level = [0] * len(caption_start_times)\n",
    "\n",
    "        high_level_topics_indexes = []\n",
    "        i = 0\n",
    "        while i < len(segment_end_times):\n",
    "            end = segment_end_times[i]\n",
    "            high_level_topics_indexes.append(i)\n",
    "            if segment_end_times.count(end) == 2:\n",
    "                # skip all the subtopics of this high level topic\n",
    "                i = (\n",
    "                        segment_end_times.index(end)\n",
    "                        + segment_end_times[segment_end_times.index(end) + 1:].index(end)\n",
    "                        + 2\n",
    "                )\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "        segment_start_times_high_level = [\n",
    "            segment_start_times[i] for i in high_level_topics_indexes\n",
    "        ]\n",
    "\n",
    "        # we skip first and last labaled segment cause they are naive segments\n",
    "        for sst in segment_start_times_high_level[1:]:\n",
    "            try:\n",
    "                topic_change_index = caption_start_times.index(sst)\n",
    "            except ValueError:\n",
    "                topic_change_index = bisect(caption_start_times, sst)\n",
    "                if topic_change_index == len(meeting_labels_top_level):\n",
    "                    topic_change_index -= 1  # bisect my go out of boundary\n",
    "            meeting_labels_top_level[topic_change_index] = 1\n",
    "\n",
    "        labels_top_level[meeting_id] = meeting_labels_top_level\n",
    "\n",
    "        logging.info(\"MEETING TRANSCRIPTS\")\n",
    "        for i, sentence in enumerate(meeting_sentences):\n",
    "            if meeting_labels_top_level[i] == 1:\n",
    "                logging.info(\"\\n\\n<<------ Topic Change () ------>>\\n\")\n",
    "            logging.info(sentence)\n",
    "\n",
    "    return labels_top_level\n",
    "\n",
    "\n",
    "MEETING_ID_COL_NAME = \"meeting_id\"\n",
    "START_COL_NAME = \"st\"\n",
    "EN_COL_NAME = \"en\"\n",
    "CAPTION_COL_NAME = \"caption\"\n",
    "\n",
    "\n",
    "def eval_topic_segmentation(\n",
    "        input_df: pd.DataFrame,\n",
    "        label_df: pd.DataFrame,\n",
    "        topic_segmentation_algorithm: TopicSegmentationAlgorithm,\n",
    "        topic_segmentation_config: TopicSegmentationConfig,\n",
    ") -> Dict[str, float]:\n",
    "\n",
    "    prediction_segmentations = topic_segmentation(\n",
    "        topic_segmentation_algorithm,\n",
    "        input_df,\n",
    "        MEETING_ID_COL_NAME,\n",
    "        START_COL_NAME,\n",
    "        EN_COL_NAME,\n",
    "        CAPTION_COL_NAME,\n",
    "        topic_segmentation_config,\n",
    "    )\n",
    "\n",
    "    flattened = binary_labels_flattened(\n",
    "        input_df,\n",
    "        label_df,\n",
    "        MEETING_ID_COL_NAME,\n",
    "        START_COL_NAME,\n",
    "        EN_COL_NAME,\n",
    "        CAPTION_COL_NAME,\n",
    "    )\n",
    "\n",
    "    top_level = binary_labels_top_level(\n",
    "        input_df,\n",
    "        label_df,\n",
    "        MEETING_ID_COL_NAME,\n",
    "        START_COL_NAME,\n",
    "        EN_COL_NAME,\n",
    "        CAPTION_COL_NAME,\n",
    "    )\n",
    "\n",
    "    flattened_metrics = compute_metrics(\n",
    "        prediction_segmentations, flattened, metric_name_suffix=\"flattened\"\n",
    "    )\n",
    "    top_level_metrics = compute_metrics(\n",
    "        prediction_segmentations, top_level, metric_name_suffix=\"top_level\"\n",
    "    )\n",
    "\n",
    "    def merge_metrics(*metrics):\n",
    "        res = {}\n",
    "        for m in metrics:\n",
    "            for k, v in m.items():\n",
    "                res[k] = v\n",
    "        return res\n",
    "\n",
    "    return merge_metrics(flattened_metrics, top_level_metrics)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def preprocessing(df, caption_col_name):\n",
    "    fillers = [\"um\", \"uh\", \"oh\", \"hmm\", \"you know\", \"like\"]\n",
    "    fillers += list(\n",
    "        map(lambda filler: filler + \" \", fillers)\n",
    "    )  # filler inside caption with other words\n",
    "    fillers = list(\n",
    "        map(lambda filler: \"(?i)\" + filler, fillers)\n",
    "    )  # make it case-insensitive\n",
    "    df[caption_col_name].replace(fillers, [\"\"] * len(fillers), regex=True, inplace=True)\n",
    "\n",
    "    captions_with_multiple_setences = len(df.loc[df[caption_col_name].isin([\".\"])])\n",
    "    if captions_with_multiple_setences > 0:\n",
    "        print(\n",
    "            f\"WARNING: Found {captions_with_multiple_setences} captions with multiple sentences; sentence embeddings may be inaccurate.\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "\n",
    "    df = df[df[caption_col_name].str.len() > 20]\n",
    "    df.reset_index(inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def icsi_dataset():\n",
    "    pass\n",
    "\n",
    "\n",
    "def ami_dataset():\n",
    "    \"\"\"See XXXX for label generation and XXXX for input analysis\n",
    "\n",
    "            SELECT\n",
    "                fb_meeting_id AS meeting_id,\n",
    "                st,\n",
    "                en,\n",
    "                caption,\n",
    "                speaker\n",
    "            FROM {ami}\n",
    "            WHERE ds = '2021-01-12'\n",
    "\n",
    "            SELECT\n",
    "                fb_meeting_id AS meeting_id,\n",
    "                st,\n",
    "                en,\n",
    "                topic\n",
    "            FROM {labels}\n",
    "            WHERE ds = '2021-01-10'\n",
    "    \"\"\"\n",
    "    train = pd.read_csv('data/train_ami.csv')\n",
    "    test = pd.read_csv('data/test_ami.csv')\n",
    "    # train = train[train['meeting_id'] < 1]\n",
    "    # test = test[test['meeting_id'] < 1]\n",
    "    train = preprocessing(train, 'caption')\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def measure(f):\n",
    "    start_time = time.perf_counter()\n",
    "    f()\n",
    "    end_time = time.perf_counter()\n",
    "\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"Elapsed time:\", elapsed_time)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Found 4 captions with multiple sentences; sentence embeddings may be inaccurate.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meeting_id -> task_idx\n",
      "0 -> 0\n",
      "1 -> 1\n",
      "2 -> 2\n",
      "3 -> 3\n",
      "4 -> 4\n",
      "5 -> 5\n",
      "6 -> 6\n",
      "7 -> 7\n",
      "8 -> 8\n",
      "9 -> 9\n",
      "10 -> 10\n",
      "11 -> 11\n",
      "12 -> 12\n",
      "13 -> 13\n",
      "14 -> 14\n",
      "15 -> 15\n",
      "16 -> 16\n",
      "17 -> 17\n",
      "18 -> 18\n",
      "19 -> 19\n",
      "{0: [13], 1: [160, 282, 369], 2: [23, 102, 128, 160, 254, 309], 3: [14, 69], 4: [135, 189, 303], 5: [221, 239], 6: [78, 195], 7: [289], 8: [156, 244, 312, 418], 9: [54, 61, 113], 10: [260, 298, 326], 11: [24, 204], 12: [39, 315, 501, 542], 13: [20, 38, 65, 97, 119], 14: [410], 15: [337], 16: [19, 82], 17: [116], 18: [45, 250, 267], 19: [31, 49, 134, 430]}\n",
      "expected: {0: [72, 86], 1: [35, 52, 107, 269, 296], 2: [16, 42, 238, 370], 3: [8, 15, 26, 61, 73], 4: [129, 266], 5: [34, 143, 179, 266, 282, 309, 348], 6: [15, 93, 324], 7: [10, 59, 121, 190, 286], 8: [245, 506], 9: [79, 102], 10: [249, 288, 325, 342], 11: [75], 12: [95, 216, 320], 13: [105, 119, 137, 175, 233], 14: [40, 61, 227, 370, 388], 15: [26, 82, 219, 357], 16: [103, 191], 17: [64, 138, 149, 190, 220, 256], 18: [166, 258], 19: [70, 87, 113, 335]}\n",
      "Pk on 20 meetings: 0.4549938830797992\n",
      "WinDiff on 20 meetings: 0.5396218284735774\n",
      "{0: [13], 1: [160, 282, 369], 2: [23, 102, 128, 160, 254, 309], 3: [14, 69], 4: [135, 189, 303], 5: [221, 239], 6: [78, 195], 7: [289], 8: [156, 244, 312, 418], 9: [54, 61, 113], 10: [260, 298, 326], 11: [24, 204], 12: [39, 315, 501, 542], 13: [20, 38, 65, 97, 119], 14: [410], 15: [337], 16: [19, 82], 17: [116], 18: [45, 250, 267], 19: [31, 49, 134, 430]}\n",
      "expected: {0: [72, 86], 1: [35, 52, 107, 269, 296], 2: [16, 42, 238, 370], 3: [8, 15, 26, 61, 73], 4: [129, 266], 5: [34, 143, 179, 266, 282, 309, 348], 6: [15, 93, 324], 7: [10, 59, 121, 190, 286], 8: [245, 506], 9: [79, 102], 10: [249, 288, 325, 342], 11: [75], 12: [95, 216, 320], 13: [105, 119, 137, 175, 233], 14: [40, 61, 227, 370, 388], 15: [26, 82, 219, 357], 16: [103, 191], 17: [64, 138, 149, 190, 220, 256], 18: [166, 258], 19: [70, 87, 113, 335]}\n",
      "Pk on 20 meetings: 0.4549938830797992\n",
      "WinDiff on 20 meetings: 0.5396218284735774\n",
      "Elapsed time: 65.8751961250091\n"
     ]
    }
   ],
   "source": [
    "def test_ami():\n",
    "    input_df, label_df = ami_dataset()\n",
    "    eval_topic_segmentation(\n",
    "        input_df,\n",
    "        label_df,\n",
    "        TopicSegmentationAlgorithm.RANDOM,\n",
    "        TopicSegmentationConfig(TextTilingHyperparameters(TOPIC_CHANGE_THRESHOLD=0.7, SMOOTHING_WINDOW=2),\n",
    "                                MAX_SEGMENTS_CAP=False),\n",
    "    )\n",
    "\n",
    "\n",
    "measure(test_ami)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
