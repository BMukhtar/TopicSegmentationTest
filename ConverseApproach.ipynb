{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/akvelon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import attr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize.texttiling import TextTilingTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def load_sentence_transformer(model_name='all-MiniLM-L6-v2'):\n",
    "    model = SentenceTransformer(model_name)\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_spacy():\n",
    "    return spacy.load('en_core_web_sm')\n",
    "\n",
    "model = load_sentence_transformer()\n",
    "nlp = load_spacy()\n",
    "nltk.download('stopwords')\n",
    "\n",
    "input_df = pd.read_csv('./train.csv')\n",
    "label_df = pd.read_csv('./test.csv')\n",
    "input_df = input_df[input_df['meeting_id'] < 1]\n",
    "label_df = label_df[label_df['meeting_id'] < 1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "32"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "@attr.s\n",
    "class SemanticTextSegmentation:\n",
    "\n",
    "    \"\"\"\n",
    "    Segment a call transcript based on topics discussed in the call using\n",
    "    TextTilling with Sentence Similarity via sentence transformer.\n",
    "\n",
    "    Paramters\n",
    "    ---------\n",
    "    data: pd.Dataframe\n",
    "        Pass the trascript in the dataframe format\n",
    "\n",
    "    utterance: str\n",
    "        pass the column name which represent utterance in transcript dataframe\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    data = attr.ib()\n",
    "    utterance = attr.ib(default='utterance')\n",
    "\n",
    "    def __attrs_post_init__(self):\n",
    "        columns = self.data.columns.tolist()\n",
    "\n",
    "    def get_segments_ind(self, threshold=0.7):\n",
    "        segments = self._text_tilling()\n",
    "        merge_index = self._merge_segments(segments, threshold)\n",
    "        return merge_index\n",
    "\n",
    "    def get_segments(self, threshold=0.7):\n",
    "        \"\"\"\n",
    "        returns the transcript segments computed with texttiling and sentence-transformer.\n",
    "\n",
    "        Paramters\n",
    "        ---------\n",
    "        threshold: float\n",
    "            sentence similarity threshold. (used to merge the sentences into coherant segments)\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        new_segments: list\n",
    "            list of segments\n",
    "        \"\"\"\n",
    "        segments = self._text_tilling()\n",
    "        merge_index = self._index_mapping(self._merge_segments(segments, threshold))\n",
    "        new_segments = []\n",
    "        for i in merge_index:\n",
    "            seg = ' '.join([segments[_] for _ in i])\n",
    "            new_segments.append(seg)\n",
    "        return new_segments\n",
    "\n",
    "    def _merge_segments(self, segments, threshold):\n",
    "        segment_map = [0]\n",
    "        sims = []\n",
    "        for index, (text1, text2) in enumerate(zip(segments[:-1], segments[1:])):\n",
    "            sim = self._get_similarity(text1, text2)\n",
    "            sims.append(sim)\n",
    "\n",
    "        # threshold = np.mean(sims) - 3 * np.var(sims)\n",
    "        for sim in sims:\n",
    "            if sim >= threshold:\n",
    "                segment_map.append(0)\n",
    "            else:\n",
    "                segment_map.append(1)\n",
    "        return segment_map\n",
    "\n",
    "    def _index_mapping(self, segment_map):\n",
    "        index_list = []\n",
    "        temp = []\n",
    "        for index, i in enumerate(segment_map):\n",
    "            if i == 1:\n",
    "                index_list.append(temp)\n",
    "                temp = [index]\n",
    "            else:\n",
    "                temp.append(index)\n",
    "        index_list.append(temp)\n",
    "        return index_list\n",
    "\n",
    "    def _get_similarity(self, text1, text2):\n",
    "        sentence_1 = [i.text.strip()\n",
    "                      for i in nlp(text1).sents if len(i.text.split(' ')) > 1]\n",
    "        sentence_2 = [i.text.strip()\n",
    "                      for i in nlp(text2).sents if len(i.text.split(' ')) > 2]\n",
    "        embeding_1 = model.encode(sentence_1)\n",
    "        embeding_2 = model.encode(sentence_2)\n",
    "        embeding_1 = np.mean(embeding_1, axis=0).reshape(1, -1)\n",
    "        embeding_2 = np.mean(embeding_2, axis=0).reshape(1, -1)\n",
    "\n",
    "        if np.any(np.isnan(embeding_1)) or np.any(np.isnan(embeding_2)):\n",
    "            return 1\n",
    "\n",
    "        sim = cosine_similarity(embeding_1, embeding_2)\n",
    "        return sim\n",
    "\n",
    "    def _text_tilling(self):\n",
    "        tt = TextTilingTokenizer(w=15, k=10)\n",
    "        text = '\\n\\n\\t'.join(self.data[self.utterance].tolist())\n",
    "        segment = tt.tokenize(text)\n",
    "        segment = [i.replace(\"\\n\\n\\t\", ' ') for i in segment]\n",
    "        return segment\n",
    "\n",
    "\n",
    "\n",
    "segmenter = SemanticTextSegmentation(input_df, 'caption')\n",
    "binary = segmenter.get_segments_ind(threshold=0.5)\n",
    "len(binary)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "[0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0]"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "9"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segments = segmenter.get_segments(threshold=0.5)\n",
    "len(segments)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "\"Here we go . Welcome everybody . Um , I'm Abigail Claflin . You can call me Abbie . 'S see . PowerPoint , that's not it . There we go . So this is our kick off meeting . Um and I guess we should all get acquainted {vocalsound} . {vocalsound} Let's {disfmarker} shall we all introduce ourselves ? Hi I'm Chiara , I'm the um Marketing Expert . Um , would you like me to talk about my aims at the moment , or would you like me to just say my name and then we can talk about business later ? I think we'll get around to that , yeah . So this is just introductions yeah . We'll get round to that later .  My name is Chiara and I'm the Marketing Expert . Okay . I forgot to s say I'm the Project Manager but I figured you all knew that already , {vocalsound} {vocalsound} {vocalsound} um so . {vocalsound} I'm Stephanie and I am the User Interface Designer . I'm Krista and I'm the Industrial Designer . Okay . Um so f here's our agenda for today . Um we're gonna do some tool training , project plan and discuss then close .  {vocalsound} Um so . So our aim is to produce a remote control that is original , trendy and user friendly . And to do this , we have to {disfmarker} um there's certain things we have to consider about functional aspects and conceptual design of the thing . So . We'll get to that . Oh there it is . Right . Functional design , conceptual design and detailed design .\""
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segments[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
