{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Uncomment below line if this library is missing\n",
    "# !python -m spacy download en_core_web_sm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/akvelon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import attr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize.texttiling import TextTilingTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def load_sentence_transformer(model_name='all-MiniLM-L6-v2'):\n",
    "    model = SentenceTransformer(model_name)\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_spacy():\n",
    "    return spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "model = load_sentence_transformer()\n",
    "nlp = load_spacy()\n",
    "nltk.download('stopwords')\n",
    "\n",
    "input_df = pd.read_csv('./data/train_ami.csv')\n",
    "label_df = pd.read_csv('./data/test_ami.csv')\n",
    "input_df = input_df[input_df['meeting_id'] < 1]  # for performance test only first meeting\n",
    "label_df = label_df[label_df['meeting_id'] < 1]  # for performance test only first meeting"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "\n",
    "@attr.s\n",
    "class SemanticTextSegmentation:\n",
    "    \"\"\"\n",
    "    Segment a call transcript based on topics discussed in the call using\n",
    "    TextTilling with Sentence Similarity via sentence transformer.\n",
    "\n",
    "    Paramters\n",
    "    ---------\n",
    "    data: pd.Dataframe\n",
    "        Pass the trascript in the dataframe format\n",
    "\n",
    "    utterance: str\n",
    "        pass the column name which represent utterance in transcript dataframe\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    data = attr.ib()\n",
    "    utterance = attr.ib(default='utterance')\n",
    "\n",
    "    def __attrs_post_init__(self):\n",
    "        columns = self.data.columns.tolist()\n",
    "\n",
    "    def get_segments_ind(self, threshold=0.7):\n",
    "        segments = self._text_tilling()\n",
    "        merge_index = self._merge_segments(segments, threshold)\n",
    "        return merge_index\n",
    "\n",
    "    def get_segments(self, threshold=0.7):\n",
    "        \"\"\"\n",
    "        returns the transcript segments computed with texttiling and sentence-transformer.\n",
    "\n",
    "        Paramters\n",
    "        ---------\n",
    "        threshold: float\n",
    "            sentence similarity threshold. (used to merge the sentences into coherant segments)\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        new_segments: list\n",
    "            list of segments\n",
    "        \"\"\"\n",
    "        segments = self._text_tilling()\n",
    "        merge_index = self._index_mapping(self._merge_segments(segments, threshold))\n",
    "        new_segments = []\n",
    "        for i in merge_index:\n",
    "            seg = ' '.join([segments[_] for _ in i])\n",
    "            new_segments.append(seg)\n",
    "        return new_segments\n",
    "\n",
    "    def _merge_segments(self, segments, threshold):\n",
    "        segment_map = [0]\n",
    "        sims = []\n",
    "        for index, (text1, text2) in enumerate(zip(segments[:-1], segments[1:])):\n",
    "            sim = self._get_similarity(text1, text2)\n",
    "            sims.append(sim)\n",
    "\n",
    "        # threshold = np.mean(sims) - 3 * np.var(sims)\n",
    "        for sim in sims:\n",
    "            if sim >= threshold:\n",
    "                segment_map.append(0)\n",
    "            else:\n",
    "                segment_map.append(1)\n",
    "        return segment_map\n",
    "\n",
    "    def _index_mapping(self, segment_map):\n",
    "        index_list = []\n",
    "        temp = []\n",
    "        for index, i in enumerate(segment_map):\n",
    "            if i == 1:\n",
    "                index_list.append(temp)\n",
    "                temp = [index]\n",
    "            else:\n",
    "                temp.append(index)\n",
    "        index_list.append(temp)\n",
    "        return index_list\n",
    "\n",
    "    def _get_similarity(self, text1, text2):\n",
    "        sentence_1 = [i.text.strip()\n",
    "                      for i in nlp(text1).sents if len(i.text.split(' ')) > 1]\n",
    "        sentence_2 = [i.text.strip()\n",
    "                      for i in nlp(text2).sents if len(i.text.split(' ')) > 2]\n",
    "        embeding_1 = model.encode(sentence_1)\n",
    "        embeding_2 = model.encode(sentence_2)\n",
    "        embeding_1 = np.mean(embeding_1, axis=0).reshape(1, -1)\n",
    "        embeding_2 = np.mean(embeding_2, axis=0).reshape(1, -1)\n",
    "\n",
    "        if np.any(np.isnan(embeding_1)) or np.any(np.isnan(embeding_2)):\n",
    "            return 1\n",
    "\n",
    "        sim = cosine_similarity(embeding_1, embeding_2)\n",
    "        return sim\n",
    "\n",
    "    def _text_tilling(self):\n",
    "        tt = TextTilingTokenizer(w=15, k=10)\n",
    "        text = '\\n\\n\\t'.join(self.data[self.utterance].tolist())\n",
    "        segment = tt.tokenize(text)\n",
    "        segment = [i.replace(\"\\n\\n\\t\", ' ') for i in segment]\n",
    "        return segment\n",
    "\n",
    "# segmenter = SemanticTextSegmentation(input_df, 'caption')\n",
    "# binary = segmenter.get_segments_ind(threshold=0.5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# segments = segmenter.get_segments(threshold=0.5)\n",
    "# print('\\n\\n<-- Topic Change -->\\n\\n'.join(segments[:5]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "def convert_time_to_seconds(time_str):\n",
    "    time_parts = time_str.split(':')\n",
    "    hours = 0\n",
    "    minutes = 0\n",
    "    seconds = 0\n",
    "    milliseconds = 0\n",
    "\n",
    "    if len(time_parts) == 2:\n",
    "        minutes, seconds = map(float, time_parts)\n",
    "    elif len(time_parts) == 3:\n",
    "        hours, minutes, seconds = map(float, time_parts)\n",
    "\n",
    "    seconds += (hours * 3600) + (minutes * 60)\n",
    "\n",
    "    return seconds\n",
    "\n",
    "\n",
    "def vvt_to_df(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = f.read()\n",
    "    meeting_id = file_path  # assuming meeting ID is in the file name and just an integer\n",
    "    utterances = re.findall(r'(\\d?\\d?:?\\d+:\\d+.\\d+).+?(\\d?\\d?:?\\d+:\\d+.\\d+)\\n(.*?)\\n\\n', data, re.DOTALL)\n",
    "    result = []\n",
    "    utterance_id = 1\n",
    "    for utterance in utterances:\n",
    "        st, en, caption = utterance\n",
    "        st_sec = convert_time_to_seconds(st)\n",
    "        en_sec = convert_time_to_seconds(en)\n",
    "        result.append(\n",
    "            {'meeting_id': meeting_id, 'st': st_sec, 'en': en_sec, 'caption': caption, 'speaker': utterance_id})\n",
    "        utterance_id += 1\n",
    "    return pd.DataFrame(result)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     meeting_id     st     en  \\\n",
      "0  ./data/episode_001_large.vtt   0.00   4.20   \n",
      "1  ./data/episode_001_large.vtt   4.20   6.60   \n",
      "2  ./data/episode_001_large.vtt   6.60   8.68   \n",
      "3  ./data/episode_001_large.vtt   8.68  11.92   \n",
      "4  ./data/episode_001_large.vtt  11.92  16.96   \n",
      "\n",
      "                                             caption  speaker  \n",
      "0   As part of MIT course 6S099, Artificial Gener...        1  \n",
      "1   I've gotten the chance to sit down with Max T...        2  \n",
      "2                     He is a professor here at MIT.        3  \n",
      "3   He's a physicist, spent a large part of his c...        4  \n",
      "4   studying the mysteries of our cosmological un...        5  \n"
     ]
    }
   ],
   "source": [
    "file_path = './data/episode_001_large.vtt'\n",
    "df = vvt_to_df(file_path)\n",
    "print(df.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "data": {
      "text/plain": "                        meeting_id       st       en  \\\n1775  ./data/episode_001_large.vtt  4951.44  4952.56   \n1776  ./data/episode_001_large.vtt  4952.56  4953.56   \n1777  ./data/episode_001_large.vtt  4953.56  4954.40   \n1778  ./data/episode_001_large.vtt  4954.40  4955.24   \n1779  ./data/episode_001_large.vtt  4955.24  4960.24   \n\n                              caption  speaker  \n1775   Thank you for your time today.     1776  \n1776               It's been awesome.     1777  \n1777               Thank you so much.     1778  \n1778                          Thanks.     1779  \n1779                Have a great day.     1780  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>meeting_id</th>\n      <th>st</th>\n      <th>en</th>\n      <th>caption</th>\n      <th>speaker</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1775</th>\n      <td>./data/episode_001_large.vtt</td>\n      <td>4951.44</td>\n      <td>4952.56</td>\n      <td>Thank you for your time today.</td>\n      <td>1776</td>\n    </tr>\n    <tr>\n      <th>1776</th>\n      <td>./data/episode_001_large.vtt</td>\n      <td>4952.56</td>\n      <td>4953.56</td>\n      <td>It's been awesome.</td>\n      <td>1777</td>\n    </tr>\n    <tr>\n      <th>1777</th>\n      <td>./data/episode_001_large.vtt</td>\n      <td>4953.56</td>\n      <td>4954.40</td>\n      <td>Thank you so much.</td>\n      <td>1778</td>\n    </tr>\n    <tr>\n      <th>1778</th>\n      <td>./data/episode_001_large.vtt</td>\n      <td>4954.40</td>\n      <td>4955.24</td>\n      <td>Thanks.</td>\n      <td>1779</td>\n    </tr>\n    <tr>\n      <th>1779</th>\n      <td>./data/episode_001_large.vtt</td>\n      <td>4955.24</td>\n      <td>4960.24</td>\n      <td>Have a great day.</td>\n      <td>1780</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " As part of MIT course 6S099, Artificial General Intelligence,  I've gotten the chance to sit down with Max Tegmark.  He is a professor here at MIT.  He's a physicist, spent a large part of his career  studying the mysteries of our cosmological universe.  But he's also studied and delved into the beneficial  possibilities and the existential risks  of artificial intelligence.  Amongst many other things, he is the cofounder  of the Future of Life Institute, author of two books,  both of which I highly recommend.  First, Our Mathematical Universe.  Second is Life 3.0.  He's truly an out of the box thinker and a fun personality,  so I really enjoy talking to him.  If you'd like to see more of these videos in the future,  please subscribe and also click the little bell icon  to make sure you don't miss any videos.  Also, Twitter, LinkedIn, agi.mit.edu  if you wanna watch other lectures  or conversations like this one.  Better yet, go read Max's book, Life 3.0.  Chapter seven on goals is my favorite.  It's really where philosophy and engineering come together  and it opens with a quote by Dostoevsky.   The mystery of human existence lies not in just staying alive  but in finding something to live for.  Lastly, I believe that every failure rewards us  with an opportunity to learn  and in that sense, I've been very fortunate  to fail in so many new and exciting ways  and this conversation was no different.  I've learned about something called\n",
      "\n",
      "<-- Topic Change -->\n",
      "\n",
      "  radio frequency interference, RFI, look it up.  Apparently, music and conversations  from local radio stations can bleed into the audio  that you're recording in such a way  that it almost completely ruins that audio.  It's an exceptionally difficult sound source to remove.  So, I've gotten the opportunity to learn  how to avoid RFI in the future during recording sessions.  I've also gotten the opportunity to learn  how to use Adobe Audition and iZotope RX 6  to do some noise, some audio repair.  Of course, this is an exceptionally difficult noise  to remove.  I am an engineer.  I'm not an audio engineer.  Neither is anybody else in our group  but we did our best.  Nevertheless, I thank you for your patience\n",
      "\n",
      "<-- Topic Change -->\n",
      "\n",
      "  and I hope you're still able to enjoy this conversation.  Do you think there's intelligent life  out there in the universe?  Let's open up with an easy question.  I have a minority view here actually.  When I give public lectures, I often ask for a show of hands  who thinks there's intelligent life out there somewhere else  and almost everyone put their hands up  and when I ask why, they'll be like,  oh, there's so many galaxies out there, there's gotta be.   But I'm a numbers nerd, right?  So when you look more carefully at it,  it's not so clear at all.  When we talk about our universe, first of all,  we don't mean all of space.  We actually mean, I don't know,  you can throw me the universe if you want,  it's behind you there.  It's, we simply mean the spherical region of space  from which light has a time to reach us so far  during the 14.8 billion year,  13.8 billion years since our Big Bang.  There's more space here but this is what we call a universe  because that's all we have access to.  So is there intelligent life here  that's gotten to the point of building telescopes  and computers?   My guess is no, actually.  The probability of it happening on any given planet  is some number we don't know what it is.  And what we do know is that the number can't be super high  because there's over a billion Earth like planets  in the Milky Way galaxy alone,  many of which are billions of years older than Earth.  And aside from some UFO believers,   there isn't much evidence  that any superduran civilization has come here at all.  And so that's the famous Fermi paradox, right?  And then if you work the numbers,  what you find is that if you have no clue  what the probability is of getting life on a given planet,  so it could be 10 to the minus 10, 10 to the minus 20,  or 10 to the minus two, or any power of 10  is sort of equally likely  if you wanna be really open minded,  that translates into it being equally likely   that our nearest neighbor is 10 to the 16 meters away,  10 to the 17 meters away, 10 to the 18.  By the time you get much less than 10 to the 16 already,  we pretty much know there is nothing else that close.  And when you get beyond 10.  Because they would have discovered us.  Yeah, they would have been discovered as long ago,  or if they're really close,  we would have probably noted some engineering projects  that they're doing.  And if it's beyond 10 to the 26 meters,  that's already outside of here.  So my guess is actually that we are the only life in here\n",
      "\n",
      "<-- Topic Change -->\n",
      "\n",
      "  that's gotten the point of building advanced tech,  which I think is very,  puts a lot of responsibility on our shoulders, not screw up.  I think people who take for granted  that it's okay for us to screw up,  have an accidental nuclear war or go extinct somehow  because there's a sort of Star Trek like situation out there  where some other life forms are gonna come and bail us out   and it doesn't matter as much.  I think they're leveling us into a false sense of security.  I think it's much more prudent to say,  let's be really grateful  for this amazing opportunity we've had  and make the best of it just in case it is down to us.  So from a physics perspective,  do you think intelligent life,  so it's unique from a sort of statistical view  of the size of the universe,  but from the basic matter of the universe,  how difficult is it for intelligent life to come about?  The kind of advanced tech building life  is implied in your statement that it's really difficult  to create something like a human species.  Well, I think what we know is that going from no life  to having life that can do a level of tech,  there's some sort of two going beyond that  than actually settling our whole universe with life.  There's some major roadblock there,  which is some great filter as it's sometimes called,  which is tough to get through.  It's either that roadblock is either behind us  or in front of us.  I'm hoping very much that it's behind us.   I'm super excited every time we get a new report from NASA  saying they failed to find any life on Mars.  I'm like, yes, awesome.  Because that suggests that the hard part,  maybe it was getting the first ribosome  or some very low level kind of stepping stone  so that we're home free.  Because if that's true,  then the future is really only limited  by our own imagination.  It would be much suckier if it turns out  that this level of life is kind of a dime a dozen,  but maybe there's some other problem.  Like as soon as a civilization gets advanced technology,  within a hundred years,  they get into some stupid fight with themselves and poof.  That would be a bummer.   Yeah, so you've explored the mysteries of the universe,  the cosmological universe, the one that's sitting  between us today.  I think you've also begun to explore the other universe,  which is sort of the mystery,  the mysterious universe of the mind of intelligence,  of intelligent life.  So is there a common thread between your interest   or the way you think about space and intelligence?  Oh yeah, when I was a teenager,  I was already very fascinated by the biggest questions.  And I felt that the two biggest mysteries of all in science  were our universe out there and our universe in here.  So it's quite natural after having spent  a quarter of a century on my career,  thinking a lot about this one,  that I'm now indulging in the luxury  of doing research on this one.  It's just so cool.  I feel the time is ripe now   for you trans greatly deepening our understanding of this.  Just start exploring this one.  Yeah, because I think a lot of people view intelligence  as something mysterious that can only exist  in biological organisms like us,  and therefore dismiss all talk  about artificial general intelligence as science fiction.  But from my perspective as a physicist,  I am a blob of quarks and electrons  moving around in a certain pattern  and processing information in certain ways.   And this is also a blob of quarks and electrons.  I'm not smarter than the water bottle  because I'm made of different kinds of quarks.  I'm made of up quarks and down quarks,  exact same kind as this.  There's no secret sauce, I think, in me.  It's all about the pattern of the information processing.  And this means that there's no law of physics  saying that we can't create technology,  which can help us by being incredibly intelligent  and help us crack mysteries that we couldn't.   In other words, I think we've really only seen  the tip of the intelligence iceberg so far.  Yeah, so the perceptronium.  Yeah.  So you coined this amazing term.  It's a hypothetical state of matter,  sort of thinking from a physics perspective,  what is the kind of matter that can help,  as you're saying, subjective experience emerge,  consciousness emerge.   So how do you think about consciousness  from this physics perspective?  Very good question.  So again, I think many people have underestimated  our ability to make progress on this  by convincing themselves it's hopeless  because somehow we're missing some ingredient that we need.  There's some new consciousness particle or whatever.  I happen to think that we're not missing anything  and that it's not the interesting thing  about consciousness that gives us  this amazing subjective experience of colors  and sounds and emotions.  It's rather something at the higher level  about the patterns of information processing.  And that's why I like to think about this idea  of perceptronium.  What does it mean for an arbitrary physical system  to be conscious in terms of what its particles are doing  or its information is doing?  I don't think, I hate carbon chauvinism,  this attitude you have to be made of carbon atoms   to be smart or conscious.  There's something about the information processing  that this kind of matter performs.  Yeah, and you can see I have my favorite equations here  describing various fundamental aspects of the world.  I feel that I think one day,  maybe someone who's watching this will come up  with the equations that information processing   has to satisfy to be conscious.  I'm quite convinced there is big discovery  to be made there because let's face it,  we know that so many things are made up of information.  We know that some information processing is conscious  because we are conscious.  But we also know that a lot of information processing  is not conscious.   Like most of the information processing happening  in your brain right now is not conscious.  There are like 10 megabytes per second coming in  even just through your visual system.  You're not conscious about your heartbeat regulation  or most things.  Even if I just ask you to like read what it says here,  you look at it and then, oh, now you know what it said.   But you're not aware of how the computation actually happened.  Your consciousness is like the CEO  that got an email at the end with the final answer.  So what is it that makes a difference?  I think that's both a great science mystery.\n",
      "\n",
      "<-- Topic Change -->\n",
      "\n",
      "  We're actually studying it a little bit in my lab here  at MIT, but I also think it's just a really urgent question  to answer.  For starters, I mean, if you're an emergency room doctor  and you have an unresponsive patient coming in,  wouldn't it be great if in addition to having  a CT scanner, you had a consciousness scanner  that could figure out whether this person\n"
     ]
    }
   ],
   "source": [
    "segmenter = SemanticTextSegmentation(df, 'caption')\n",
    "segments = segmenter.get_segments(threshold=0.4)\n",
    "print('\\n\\n<-- Topic Change -->\\n\\n'.join(segments[:5]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "data": {
      "text/plain": "25"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(segments)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/akvelon/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def find_topic_timestamps(topics, sentences_df):\n",
    "    topic_timestamps = []\n",
    "\n",
    "    sentence_index = 0\n",
    "\n",
    "    for topic in topics:\n",
    "        topic_tokens = word_tokenize(topic)\n",
    "        topic_word_counts = Counter(topic_tokens)\n",
    "\n",
    "        start_time, end_time = None, None\n",
    "\n",
    "        for i, row in sentences_df[sentences_df.index >= sentence_index].iterrows():\n",
    "            sentence_tokens = [word for word in word_tokenize(row['caption']) if len(word) >= 2]\n",
    "            sentence_index += 1\n",
    "\n",
    "            if (len(sentence_tokens) - sum(topic_word_counts[token] > 0 for token in sentence_tokens)) < 2:\n",
    "                for token in sentence_tokens:\n",
    "                    topic_word_counts.subtract({token: 1})\n",
    "                if start_time is None or row['st'] < start_time:\n",
    "                    start_time = row['st']\n",
    "                if end_time is None or row['en'] > end_time:\n",
    "                    end_time = row['en']\n",
    "            else:\n",
    "                sentence_index -= 1\n",
    "                break\n",
    "\n",
    "        if start_time is not None and end_time is not None:\n",
    "            topic_timestamps.append((start_time, end_time))\n",
    "\n",
    "    return topic_timestamps\n",
    "\n",
    "\n",
    "ts = find_topic_timestamps(segments, df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "def convert_seconds_to_time(seconds):\n",
    "    hours = int(seconds // 3600)\n",
    "    seconds %= 3600\n",
    "    minutes = int(seconds // 60)\n",
    "    seconds %= 60\n",
    "    milliseconds = int((seconds % 1) * 1000)\n",
    "    seconds = int(seconds)\n",
    "    return f\"{hours}:{minutes:02}:{seconds:02}.{milliseconds:03}\"\n",
    "\n",
    "\n",
    "ts_formatted = np.array([list(map(convert_seconds_to_time, row)) for row in ts])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "data": {
      "text/plain": "                                              segment   start_time  \\\n0    As part of MIT course 6S099, Artificial Gener...  0:00:00.000   \n1     radio frequency interference, RFI, look it u...  0:01:36.159   \n2     and I hope you're still able to enjoy this c...  0:02:25.039   \n3     that's gotten the point of building advanced...  0:05:05.800   \n4     We're actually studying it a little bit in m...  0:12:05.120   \n5     is actually having locked in syndrome  or is...  0:12:27.919   \n6     who say that actually some information proce...  0:14:39.519   \n7     it's like when you're in a relationship and ...  0:15:45.039   \n8     between everything seeming like there's cons...  0:16:03.519   \n9     or women don't have souls or whatever.  So I...  0:17:39.480   \n10    Maybe it's a good assumption.  We should bui...  0:26:47.960   \n11    When I go to a faculty meeting here,  and we...  0:41:32.920   \n12    We're not doing that even.  Look at anyone w...  0:47:24.360   \n13    You don't think there is something with the ...  0:51:41.360   \n14    but then even just computing what's going to...  0:53:17.079   \n15    I think it's all of the above.  The natural ...  0:55:18.639   \n16    You go talk to Demis Hassabis,  I know other...  0:56:13.760   \n17    still not gonna really help.  So I think the...  0:56:42.519   \n18    And I think even in the very short term,  if...  0:58:12.199   \n19    is a list of more numbers than there are ato...  1:02:49.440   \n20    you have some loss function,  you have a bun...  1:07:06.239   \n21    in quantum mechanics you can actually tunnel...  1:08:06.960   \n22    So we should be, because politicians usually...  1:18:48.119   \n23    these amazing opportunities that we'll squan...  1:21:12.079   \n24    Right, yeah.  So that goes to your view of A...  1:22:17.520   \n\n       end_time  \n0   0:01:36.159  \n1   0:02:25.039  \n2   0:05:05.000  \n3   0:12:05.120  \n4   0:12:27.919  \n5   0:14:39.519  \n6   0:15:45.039  \n7   0:16:03.120  \n8   0:17:39.480  \n9   0:26:47.960  \n10  0:41:32.920  \n11  0:47:24.360  \n12  0:51:41.360  \n13  0:53:17.079  \n14  0:55:18.639  \n15  0:56:12.400  \n16  0:56:42.519  \n17  0:58:12.199  \n18  1:02:49.440  \n19  1:07:06.239  \n20  1:08:06.960  \n21  1:18:48.119  \n22  1:21:12.079  \n23  1:22:17.520  \n24  1:22:40.239  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>segment</th>\n      <th>start_time</th>\n      <th>end_time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>As part of MIT course 6S099, Artificial Gener...</td>\n      <td>0:00:00.000</td>\n      <td>0:01:36.159</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>radio frequency interference, RFI, look it u...</td>\n      <td>0:01:36.159</td>\n      <td>0:02:25.039</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>and I hope you're still able to enjoy this c...</td>\n      <td>0:02:25.039</td>\n      <td>0:05:05.000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>that's gotten the point of building advanced...</td>\n      <td>0:05:05.800</td>\n      <td>0:12:05.120</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>We're actually studying it a little bit in m...</td>\n      <td>0:12:05.120</td>\n      <td>0:12:27.919</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>is actually having locked in syndrome  or is...</td>\n      <td>0:12:27.919</td>\n      <td>0:14:39.519</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>who say that actually some information proce...</td>\n      <td>0:14:39.519</td>\n      <td>0:15:45.039</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>it's like when you're in a relationship and ...</td>\n      <td>0:15:45.039</td>\n      <td>0:16:03.120</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>between everything seeming like there's cons...</td>\n      <td>0:16:03.519</td>\n      <td>0:17:39.480</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>or women don't have souls or whatever.  So I...</td>\n      <td>0:17:39.480</td>\n      <td>0:26:47.960</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Maybe it's a good assumption.  We should bui...</td>\n      <td>0:26:47.960</td>\n      <td>0:41:32.920</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>When I go to a faculty meeting here,  and we...</td>\n      <td>0:41:32.920</td>\n      <td>0:47:24.360</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>We're not doing that even.  Look at anyone w...</td>\n      <td>0:47:24.360</td>\n      <td>0:51:41.360</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>You don't think there is something with the ...</td>\n      <td>0:51:41.360</td>\n      <td>0:53:17.079</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>but then even just computing what's going to...</td>\n      <td>0:53:17.079</td>\n      <td>0:55:18.639</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>I think it's all of the above.  The natural ...</td>\n      <td>0:55:18.639</td>\n      <td>0:56:12.400</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>You go talk to Demis Hassabis,  I know other...</td>\n      <td>0:56:13.760</td>\n      <td>0:56:42.519</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>still not gonna really help.  So I think the...</td>\n      <td>0:56:42.519</td>\n      <td>0:58:12.199</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>And I think even in the very short term,  if...</td>\n      <td>0:58:12.199</td>\n      <td>1:02:49.440</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>is a list of more numbers than there are ato...</td>\n      <td>1:02:49.440</td>\n      <td>1:07:06.239</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>you have some loss function,  you have a bun...</td>\n      <td>1:07:06.239</td>\n      <td>1:08:06.960</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>in quantum mechanics you can actually tunnel...</td>\n      <td>1:08:06.960</td>\n      <td>1:18:48.119</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>So we should be, because politicians usually...</td>\n      <td>1:18:48.119</td>\n      <td>1:21:12.079</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>these amazing opportunities that we'll squan...</td>\n      <td>1:21:12.079</td>\n      <td>1:22:17.520</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>Right, yeah.  So that goes to your view of A...</td>\n      <td>1:22:17.520</td>\n      <td>1:22:40.239</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_df = pd.DataFrame({\"start_time\": ts_formatted[:, 0], \"end_time\": ts_formatted[:, 1], \"topic\": segments})\n",
    "out_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "out_df.to_json('converge_output.json', orient='records', indent=4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
